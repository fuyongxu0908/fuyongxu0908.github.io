

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="fyxu">
  <meta name="keywords" content="">
  
    <meta name="description" content="Seq2Seq Model  Seq2Seq结构 Seq2Seq(Sequence-to-Sequence)[1,2]模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。Seq2Seq模型包含编码器(Encoder)和解码器(Decoder)两部分，它们通常是两个不同的 RNN。   Seq2Seq的训练  1、在Seq2Seq的训练过程中，编码和解码时同时训练的；编码的输入和解码的输入">
<meta property="og:type" content="article">
<meta property="og:title" content="seq2seq for personalized dialogue system">
<meta property="og:url" content="http://example.com/2022/06/12/seq2seq%20for%20personalized%20dialogue%20system/index.html">
<meta property="og:site_name" content="fyxu">
<meta property="og:description" content="Seq2Seq Model  Seq2Seq结构 Seq2Seq(Sequence-to-Sequence)[1,2]模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。Seq2Seq模型包含编码器(Encoder)和解码器(Decoder)两部分，它们通常是两个不同的 RNN。   Seq2Seq的训练  1、在Seq2Seq的训练过程中，编码和解码时同时训练的；编码的输入和解码的输入">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/jpg/s2s.png">
<meta property="article:published_time" content="2022-06-12T13:06:44.211Z">
<meta property="article:modified_time" content="2022-06-12T13:49:29.233Z">
<meta property="article:author" content="fyxu">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/jpg/s2s.png">
  
  
  
  <title>seq2seq for personalized dialogue system - fyxu</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="seq2seq for personalized dialogue system"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-06-12 21:06" pubdate>
          2022年6月12日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          23k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          193 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">seq2seq for personalized dialogue system</h1>
            
            <div class="markdown-body">
              
              <h1 id="seq2seq-model"><a class="markdownIt-Anchor" href="#seq2seq-model"></a> Seq2Seq Model</h1>
<h2 id="seq2seq结构"><a class="markdownIt-Anchor" href="#seq2seq结构"></a> Seq2Seq结构</h2>
<p>Seq2Seq(Sequence-to-Sequence)[1,2]模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。Seq2Seq模型包含编码器(Encoder)和解码器(Decoder)两部分，它们通常是两个不同的 RNN。<br>
<img src="/jpg/s2s.png" srcset="/img/loading.gif" lazyload alt="s2s.png"></p>
<h2 id="seq2seq的训练"><a class="markdownIt-Anchor" href="#seq2seq的训练"></a> Seq2Seq的训练</h2>
<h3 id="1-在seq2seq的训练过程中编码和解码时同时训练的编码的输入和解码的输入作为统一的整体输入进模型解码的输出作为整体的输出"><a class="markdownIt-Anchor" href="#1-在seq2seq的训练过程中编码和解码时同时训练的编码的输入和解码的输入作为统一的整体输入进模型解码的输出作为整体的输出"></a> 1、在Seq2Seq的训练过程中，编码和解码时同时训练的；编码的输入和解码的输入作为统一的整体输入进模型，解码的输出作为整体的输出。</h3>
<h3 id="2-rnn的训练更新有两种模式"><a class="markdownIt-Anchor" href="#2-rnn的训练更新有两种模式"></a> 2、RNN的训练更新有两种模式</h3>
<ul>
<li>free-running mode: 上一个state的输出作为下一个state的输入</li>
<li><strong>teacher-forcing mode</strong>: 使用来自先验时间步长的输出作为输入<br>
free-running mode存在的问题(1)收敛慢(2)模型不稳定(3)Poor skil；训练迭代过程早期的RNN预测能力非常弱，几乎不能给出好的生成结果。如果某一个unit产生了垃圾结果，必然会影响后面一片unit的学习。错误结果会导致后续的学习都受到不好的影响，导致学习速度变慢，难以收敛。因此出现了teacher forcing。<br>
teacher-forcing的问题: <strong>Exposure Bias</strong>，使用teacher-forcing，在训练过程中，模型会有较好的效果，但是在测试的时候因为不能得到ground truth的支持，存在训练测试偏差，模型会变得脆弱。</li>
</ul>
<h1 id="personalized-dialogue-system"><a class="markdownIt-Anchor" href="#personalized-dialogue-system"></a> Personalized Dialogue System</h1>
<h2 id="训练参数的设置设置随机种子"><a class="markdownIt-Anchor" href="#训练参数的设置设置随机种子"></a> 训练参数的设置&amp;设置随机种子</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_set_args</span>():<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--sed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1234</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;The random seed.&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--dataset_path&#x27;</span>, default=<span class="hljs-string">&#x27;&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--cache&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;./dataset_cache/&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--tokenize&#x27;</span>, default=<span class="hljs-string">&#x27;split&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--train_bs&#x27;</span>, default=<span class="hljs-number">64</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--valid_bs&#x27;</span>, default=<span class="hljs-number">32</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--epochs&#x27;</span>, default=<span class="hljs-number">400</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--device&#x27;</span>, default=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--attn_type&#x27;</span>, default=<span class="hljs-string">&#x27;general&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--rnn_class&#x27;</span>, default=<span class="hljs-string">&#x27;lstm&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--lr&#x27;</span>, default=<span class="hljs-number">3</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--momentum&#x27;</span>, default=<span class="hljs-number">0.9</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--clip&#x27;</span>, default=<span class="hljs-number">0.1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--save_path&#x27;</span>, default=<span class="hljs-string">&#x27;./ckp&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--load_from_path&#x27;</span>, default=<span class="hljs-string">&#x27;./ckp/v1/199_model.pth&#x27;</span>)<br><br>    parser.add_argument(<span class="hljs-string">&#x27;--embedding_size&#x27;</span>, default=<span class="hljs-number">256</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--hidden_size&#x27;</span>, default=<span class="hljs-number">1024</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--attn_length&#x27;</span>, default=<span class="hljs-number">48</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_layers&#x27;</span>, default=<span class="hljs-number">2</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--dropout&#x27;</span>, default=<span class="hljs-number">0.1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--attn_time&#x27;</span>, default=<span class="hljs-string">&#x27;post&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--bidirectional&#x27;</span>, default=<span class="hljs-literal">True</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--softmax_layer_bias&#x27;</span>, default=<span class="hljs-literal">False</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_softmax&#x27;</span>, default=<span class="hljs-number">1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--beam_size&#x27;</span>, default=<span class="hljs-number">1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--topk&#x27;</span>, default=<span class="hljs-number">1</span>)<br><br>    args = parser.parse_args()<br>    <span class="hljs-keyword">return</span> args<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_set_seed</span>(<span class="hljs-params">args</span>):<br>    np.random.seed(args.sed)<br>    torch.manual_seed(args.sed)<br>    torch.cuda.manual_seed_all(args.sed)<br>    np.random.seed(args.sed)<br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>
<h2 id="获取数据集构建glove所需要的词典"><a class="markdownIt-Anchor" href="#获取数据集构建glove所需要的词典"></a> 获取数据集，构建Glove所需要的词典</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><code class="hljs python">PERSONACHAT_URL = <span class="hljs-string">&quot;https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json&quot;</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DialogDict</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, corpus_path, args</span>):<br>        self.corpus_path = corpus_path<br>        self.ind2tok = &#123;&#125;<br>        self.tok2ind = &#123;&#125;<br>        self.freq = defaultdict(<span class="hljs-built_in">int</span>)<br>        self.__null__ = <span class="hljs-string">&#x27;__null__&#x27;</span><br>        self.__start__ = <span class="hljs-string">&#x27;__start__&#x27;</span><br>        self.__end__ = <span class="hljs-string">&#x27;__end__&#x27;</span><br>        self.__unk__ = <span class="hljs-string">&#x27;__unk__&#x27;</span><br>        self.args = args<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">escape</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-string">r&quot;&quot;&quot;Replace potential special characters with escaped version.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For example, \n =&gt; \\n and \t =&gt; \\t</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param s: string to escape</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> s.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;\\n&#x27;</span>).replace(<span class="hljs-string">&#x27;\t&#x27;</span>, <span class="hljs-string">&#x27;\\t&#x27;</span>).replace(<span class="hljs-string">&#x27;\r&#x27;</span>, <span class="hljs-string">&#x27;\\r&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unescape</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-string">r&quot;&quot;&quot;Revert escaped characters back to their special version.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For example, \\n =&gt; \n and \\t =&gt; \t</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param s: string to unescape</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> s.replace(<span class="hljs-string">&#x27;\\n&#x27;</span>, <span class="hljs-string">&#x27;\n&#x27;</span>).replace(<span class="hljs-string">&#x27;\\t&#x27;</span>, <span class="hljs-string">&#x27;\t&#x27;</span>).replace(<span class="hljs-string">&#x27;\\r&#x27;</span>, <span class="hljs-string">&#x27;\r&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tokenize</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-keyword">return</span> text.replace(<span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27; . &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27; , &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;;&#x27;</span>, <span class="hljs-string">&#x27; ; &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;:&#x27;</span>, <span class="hljs-string">&#x27; : &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27; ! &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;?&#x27;</span>, <span class="hljs-string">&#x27; ? &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;:&#x27;</span>, <span class="hljs-string">&#x27; : &#x27;</span>) \<br>               .split()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add_token</span>(<span class="hljs-params">self, word</span>):<br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.tok2ind:<br>            index = <span class="hljs-built_in">len</span>(self.tok2ind)<br>            self.tok2ind[word] = index<br>            self.ind2tok[index] = word<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add_to_dict</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens:<br>            self.add_token(token)<br>            self.freq[token] += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self</span>):<br>        self.tok2ind.update(&#123;<span class="hljs-string">&#x27;__null__&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;__start__&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;__end__&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;__unk__&#x27;</span>: <span class="hljs-number">3</span>&#125;)<br>        self.ind2tok.update(&#123;<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;__null__&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;__start__&#x27;</span>, <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;__end__&#x27;</span>, <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;__unk__&#x27;</span>&#125;)<br>        self.freq[self.__null__] = <span class="hljs-number">1000000003</span><br>        self.freq[self.__start__] = <span class="hljs-number">1000000002</span><br>        self.freq[self.__end__] = <span class="hljs-number">1000000001</span><br>        self.freq[self.__unk__] = <span class="hljs-number">1000000000</span><br><br>        <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(self.corpus_path, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;utf8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>                self.add_to_dict(self._tokenize(line))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save</span>(<span class="hljs-params">self</span>):<br>        file_name = self.args.cache + <span class="hljs-string">&#x27;Dialogue.dict&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Dictionary: saving dictionary to &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(file_name))<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&#x27;a&#x27;</span>) <span class="hljs-keyword">as</span> write:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.ind2tok)):<br>                tok = self.ind2tok[i]<br>                cnt = self.freq[tok]<br>                write.write(<span class="hljs-string">&#x27;&#123;tok&#125;\t&#123;cnt&#125;\n&#x27;</span>.<span class="hljs-built_in">format</span>(tok=self.escape(tok), cnt=cnt))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">self</span>):<br>        file_name = self.args.cache + <span class="hljs-string">&#x27;Dialogue.dict&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Dictionary: loading dictionary from &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(file_name))<br>        lower_special = self.__null__ == self.__null__.lower()<br>        SPECIAL_TOKENS = &#123;<span class="hljs-string">&#x27;__unk__&#x27;</span>, <span class="hljs-string">&#x27;__null__&#x27;</span>, <span class="hljs-string">&#x27;__end__&#x27;</span>, <span class="hljs-string">&#x27;__start__&#x27;</span>&#125;<br>        <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>, errors=<span class="hljs-string">&#x27;ignore&#x27;</span>) <span class="hljs-keyword">as</span> read:<br>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> read:<br>                split = line.strip().split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>                token = self.unescape(split[<span class="hljs-number">0</span>])<br>                <span class="hljs-keyword">if</span> lower_special <span class="hljs-keyword">and</span> token <span class="hljs-keyword">in</span> SPECIAL_TOKENS:<br>                    token = token.lower()<br>                cnt = <span class="hljs-built_in">int</span>(split[<span class="hljs-number">1</span>]) <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>                self.freq[token] = cnt<br>                self.add_token(token)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[ num words =  %d ]&#x27;</span> % <span class="hljs-built_in">len</span>(self.ind2tok))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_decode</span>(<span class="hljs-params">self, tokens_list</span>):<br>        <span class="hljs-keyword">return</span> [self.ind2tok[w.item()] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens_list]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_dataset</span>(<span class="hljs-params">args</span>):<br>    corpus_path = args.cache + <span class="hljs-string">&#x27;corpus.txt&#x27;</span><br>    datasetpath = args.cache + <span class="hljs-string">&#x27;dataset.json&#x27;</span><br>    dic_path = args.cache + <span class="hljs-string">&#x27;Dialogue.dict&#x27;</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(corpus_path) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> os.path.exists(datasetpath) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> os.path.exists(dic_path):<br>        dataset_path = args.dataset_path <span class="hljs-keyword">or</span> PERSONACHAT_URL<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Download dataset from %s&quot;</span>, dataset_path)<br>        personachat_file = cached_path(dataset_path)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(personachat_file, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            dataset = json.loads(f.read())<br>        corpus = codecs.<span class="hljs-built_in">open</span>(corpus_path, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;utf8&#x27;</span>)<br>        datasets = &#123;<span class="hljs-string">&quot;train&quot;</span>: [], <span class="hljs-string">&quot;valid&quot;</span>: []&#125;<br>        <span class="hljs-keyword">for</span> dataset_name, data <span class="hljs-keyword">in</span> dataset.items():<br>            <span class="hljs-keyword">for</span> dialog <span class="hljs-keyword">in</span> tqdm.tqdm(data, desc=<span class="hljs-string">&quot;Process Data&quot;</span>):<br>                persona = dialog[<span class="hljs-string">&#x27;personality&#x27;</span>].copy()<br>                <span class="hljs-keyword">for</span> utter <span class="hljs-keyword">in</span> dialog[<span class="hljs-string">&#x27;utterances&#x27;</span>]:<br>                    query = utter[<span class="hljs-string">&#x27;history&#x27;</span>][-<span class="hljs-number">1</span>]<br>                    datasets[dataset_name].append((<span class="hljs-string">&#x27;persona : &#x27;</span> + <span class="hljs-string">&#x27; &#x27;</span>.join(persona), query, utter[<span class="hljs-string">&#x27;candidates&#x27;</span>][-<span class="hljs-number">1</span>]))<br>                    corpus.write(<span class="hljs-string">&#x27;persona : &#x27;</span> + <span class="hljs-string">&#x27; &#x27;</span>.join(persona) + <span class="hljs-string">&#x27; &#x27;</span> + query + <span class="hljs-string">&#x27; &#x27;</span> + utter[<span class="hljs-string">&#x27;candidates&#x27;</span>][-<span class="hljs-number">1</span>] + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>        corpus.close()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Save Datasets...&quot;</span>)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(datasetpath, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            json.dump(datasets, f)<br>        dialog_dict = DialogDict(corpus_path, args)<br>        dialog_dict.build()<br>        dialog_dict.save()<br>    <span class="hljs-keyword">else</span>:<br>        dialog_dict = DialogDict(corpus_path, args)<br>        dialog_dict.load()<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(datasetpath, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            datasets = json.load(f)<br><br>    <span class="hljs-keyword">return</span> datasets, dialog_dict<br><br></code></pre></td></tr></table></figure>
<h2 id="构造dataset和dataloader"><a class="markdownIt-Anchor" href="#构造dataset和dataloader"></a> 构造Dataset和Dataloader</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvAI2</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dataset</span>):<br>        self.dataset = dataset<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.dataset[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tokenized</span>(<span class="hljs-params">self, seq, dialog_dict</span>):<br>        ret = []<br>        <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq.split():<br>            <span class="hljs-keyword">try</span>:<br>                ret.append(dialog_dict.tok2ind[w])<br>            <span class="hljs-keyword">except</span>:<br>                ret.append(dialog_dict.tok2ind[<span class="hljs-string">&#x27;__unk__&#x27;</span>])<br>        <span class="hljs-keyword">return</span> ret<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">self, batch, dialog_dict</span>):<br>        input_seq = [dialog_dict.__start__ + <span class="hljs-string">&#x27; &#x27;</span> + b[<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27; &#x27;</span> + b[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> batch]<br>        response_seq = [b[<span class="hljs-number">2</span>] + <span class="hljs-string">&#x27; &#x27;</span> + dialog_dict.__end__ <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> batch]<br><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(input_seq) == <span class="hljs-built_in">len</span>(response_seq)<br>        input_seq_tokenized = [self._tokenized(seq, dialog_dict) <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> input_seq]<br>        response_seq_tokenized = [self._tokenized(res, dialog_dict) <span class="hljs-keyword">for</span> res <span class="hljs-keyword">in</span> response_seq]<br><br>        sort_map = [(i, j) <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_seq_tokenized, response_seq_tokenized)]<br>        sorted_map = <span class="hljs-built_in">sorted</span>(sort_map, key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-number">0</span>]), reverse=<span class="hljs-literal">True</span>)<br><br>        input_lens = [<span class="hljs-built_in">len</span>(it[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map]<br>        res_lens = [<span class="hljs-built_in">len</span>(it[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map]<br><br>        <span class="hljs-built_in">input</span> = pad_sequence([torch.tensor(it[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map], batch_first=<span class="hljs-literal">True</span>, padding_value=dialog_dict.tok2ind[dialog_dict.__null__])<br>        res = pad_sequence([torch.tensor(it[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map], batch_first=<span class="hljs-literal">True</span>, padding_value=dialog_dict.tok2ind[dialog_dict.__null__])<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span>, res, input_lens, res_lens<br>train_dataset, valid_dataset = ConvAI2(datasets[<span class="hljs-string">&#x27;train&#x27;</span>]), ConvAI2(datasets[<span class="hljs-string">&#x27;valid&#x27;</span>])<br>train_loader, valid_loader = DataLoader(dataset=train_dataset, batch_size=args.train_bs, shuffle=<span class="hljs-literal">True</span>, collate_fn=<span class="hljs-keyword">lambda</span> x: train_dataset.collate_fn(x, dialog_dict)), \<br>                                 DataLoader(dataset=valid_dataset[:<span class="hljs-number">7021</span>], batch_size=args.valid_bs, shuffle=<span class="hljs-literal">False</span>, collate_fn=<span class="hljs-keyword">lambda</span> x: train_dataset.collate_fn(x, dialog_dict))<br></code></pre></td></tr></table></figure>
<h2 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h2>
<h3 id="encoder"><a class="markdownIt-Anchor" href="#encoder"></a> Encoder</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            num_features,</span><br><span class="hljs-params">            padding_idx=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">            rnn_class=<span class="hljs-string">&#x27;lstm&#x27;</span>,</span><br><span class="hljs-params">            emb_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            hidden_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            num_layers=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">            dropout=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">            bidirectional=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            sparse=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dropout = nn.Dropout(p=dropout)<br>        self.layers = num_layers<br>        self.bidirectional = bidirectional<br>        self.hidden_size = hidden_size<br>        self.emb_layer = nn.Embedding(<br>            num_features,<br>            emb_size,<br>            padding_idx=padding_idx,<br>            sparse=sparse)<br><br>        self.rnn = rnn_class(<br>            emb_size,<br>            hidden_size,<br>            num_layers,<br>            dropout=dropout,<br>            batch_first=<span class="hljs-literal">True</span>,<br>            bidirectional=bidirectional<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xs, input_lens=<span class="hljs-literal">None</span></span>):<br>        bsz = <span class="hljs-built_in">len</span>(xs)<br><br>        <span class="hljs-comment"># embed input tokens</span><br>        xes = self.dropout(self.emb_layer(xs))<br>        xes = pack_padded_sequence(xes, input_lens, batch_first=<span class="hljs-literal">True</span>)<br>        encoder_output, hidden = self.rnn(xes)<br>        encoder_output, _ = pad_packed_sequence(encoder_output, batch_first=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        if self.bidirectional:</span><br><span class="hljs-string">            if isinstance(self.rnn, nn.LSTM):</span><br><span class="hljs-string">                hidden = (</span><br><span class="hljs-string">                    hidden[0].view(-1, 2, bsz, self.hidden_size).max(1)[0],</span><br><span class="hljs-string">                    hidden[1].view(-1, 2, bsz, self.hidden_size).max(1)[0],</span><br><span class="hljs-string">                )# 2, bidirectional</span><br><span class="hljs-string">            else:</span><br><span class="hljs-string">                hidden = hidden.view(-1, 2, bsz, self.hidden_size).max(1)[0]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> encoder_output, hidden<br><br></code></pre></td></tr></table></figure>
<h3 id="decoder"><a class="markdownIt-Anchor" href="#decoder"></a> Decoder</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            num_features,</span><br><span class="hljs-params">            padding_idx=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">            rnn_class=<span class="hljs-string">&#x27;lstm&#x27;</span>,</span><br><span class="hljs-params">            emb_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            hidden_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            num_layers=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">            dropout=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">            bidir_input=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            sparse=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            numsoftmax=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">            softmax_layer_bias=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            attn_type=<span class="hljs-string">&#x27;none&#x27;</span>,</span><br><span class="hljs-params">            attn_length=-<span class="hljs-number">1</span>,</span><br><span class="hljs-params">            attn_time=<span class="hljs-string">&#x27;pre&#x27;</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dropout = nn.Dropout(p=dropout)<br>        self.layers = num_layers<br>        self.hsz = hidden_size<br>        self.esz = emb_size<br><br>        self.embedd_layer = nn.Embedding(num_features, emb_size, padding_idx=padding_idx, sparse=sparse)<br>        self.rnn = rnn_class(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=<span class="hljs-literal">True</span>, bidirectional=bidir_input)<br><br>        <span class="hljs-comment"># rnn output to embedding</span><br>        <span class="hljs-keyword">if</span> hidden_size != emb_size <span class="hljs-keyword">and</span> numsoftmax == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># self.o2e = RandomProjection(hidden_size, emb_size)</span><br>            <span class="hljs-comment"># other option here is to learn these weights</span><br>            self.o2e = nn.Linear(hidden_size, emb_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># no need for any transformation here</span><br>            self.o2e = <span class="hljs-keyword">lambda</span> x: x<br>        <span class="hljs-comment"># embedding to scores, use custom linear to possibly share weights</span><br>        self.e2s = nn.Linear(emb_size, num_features, bias=softmax_layer_bias)<br><br>        self.attn_type = attn_type<br>        self.attn_time = attn_time<br>        self.attention = AttentionLayer(<br>            attn_type=attn_type,<br>            hidden_size=hidden_size,<br>            emb_size=emb_size,<br>            bidirectional=bidir_input,<br>            attn_length=attn_length,<br>            attn_time=attn_time,<br>        )<br><br>        self.numsoftmax = numsoftmax<br>        <span class="hljs-keyword">if</span> numsoftmax &gt; <span class="hljs-number">1</span>:<br>            self.sofrmax = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>            self.prior = nn.Linear(hidden_size, numsoftmax, bias=<span class="hljs-literal">False</span>)<br>            self.latent = nn.Linear(hidden_size, numsoftmax * emb_size)<br>            self.activation = nn.Tanh()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xs, hidden, encoder_output, attn_mask=<span class="hljs-literal">None</span>, topk=<span class="hljs-number">1</span></span>):<br>        xes = self.dropout(self.embedd_layer(xs))<br>        <span class="hljs-keyword">if</span> self.attn_time == <span class="hljs-string">&#x27;pre&#x27;</span>:<br>            xes = self.attention(xes, hidden, encoder_output, attn_mask)<br>        <span class="hljs-keyword">if</span> xes.dim() == <span class="hljs-number">2</span>:<br>            <span class="hljs-comment"># if only one token inputted, sometimes needs unsquezing</span><br>            xes.unsqueeze_(<span class="hljs-number">1</span>)<br>        output, new_hidden = self.rnn(xes, hidden)<br>        <span class="hljs-keyword">if</span> self.attn_time == <span class="hljs-string">&#x27;post&#x27;</span>:<br>            output = self.attention(output, new_hidden, encoder_output, attn_mask)<br><br>        <span class="hljs-keyword">if</span> self.numsoftmax &gt; <span class="hljs-number">1</span>:<br>            bsz = xs.size(<span class="hljs-number">0</span>)<br>            seqlen = xs.size(<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> xs.dim() &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span><br>            latent = self.latent(output)<br>            active = self.dropout(self.activation(latent))<br>            logit = self.e2s(active.view(-<span class="hljs-number">1</span>, self.esz))<br><br>            prior_logit = self.prior(output).view(-<span class="hljs-number">1</span>, self.numsoftmax)<br>            prior = self.softmax(prior_logit)  <span class="hljs-comment"># softmax over numsoftmax&#x27;s</span><br><br>            prob = self.softmax(logit).view(bsz * seqlen, self.numsoftmax, -<span class="hljs-number">1</span>)<br>            probs = (prob * prior.unsqueeze(<span class="hljs-number">2</span>)).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>).view(bsz, seqlen, -<span class="hljs-number">1</span>)<br>            scores = probs.log()<br><br>        <span class="hljs-keyword">else</span>:<br>            e = self.dropout(self.o2e(output))<br>            scores = self.e2s(e)<br><br>            <span class="hljs-comment"># select top scoring index, excluding the padding symbol (at idx zero)</span><br>            <span class="hljs-comment"># we can do topk sampling from renoramlized softmax here, default topk=1 is greedy</span><br>            <span class="hljs-keyword">if</span> topk == <span class="hljs-number">1</span>:<br>                _max_score, idx = scores.narrow(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, scores.size(<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>).<span class="hljs-built_in">max</span>(<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">elif</span> topk &gt; <span class="hljs-number">1</span>:<br>                max_score, idx = torch.topk(<br>                    F.softmax(scores.narrow(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, scores.size(<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>), <span class="hljs-number">2</span>),<br>                    topk,<br>                    dim=<span class="hljs-number">2</span>,<br>                    <span class="hljs-built_in">sorted</span>=<span class="hljs-literal">False</span>,<br>                )<br>                probs = F.softmax(<br>                    scores.narrow(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, scores.size(<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>).gather(<span class="hljs-number">2</span>, idx), <span class="hljs-number">2</span><br>                ).squeeze(<span class="hljs-number">1</span>)<br>                dist = torch.distributions.categorical.Categorical(probs)<br>                samples = dist.sample()<br>                idx = idx.gather(-<span class="hljs-number">1</span>, samples.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)).squeeze(-<span class="hljs-number">1</span>)<br>            preds = idx.add_(<span class="hljs-number">1</span>)<br><br>            <span class="hljs-keyword">return</span> preds, scores, new_hidden<br></code></pre></td></tr></table></figure>
<h3 id="attention-module"><a class="markdownIt-Anchor" href="#attention-module"></a> Attention Module</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AttentionLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            attn_type,</span><br><span class="hljs-params">            hidden_size,</span><br><span class="hljs-params">            emb_size,</span><br><span class="hljs-params">            bidirectional=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            attn_length=-<span class="hljs-number">1</span>,</span><br><span class="hljs-params">            attn_time=<span class="hljs-string">&#x27;pre&#x27;</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.attention = attn_type<br><br>        <span class="hljs-keyword">if</span> self.attention != <span class="hljs-string">&#x27;none&#x27;</span>:<br>            hsz = hidden_size<br>            hszXdirs = hsz * (<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> bidirectional <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> attn_time == <span class="hljs-string">&#x27;pre&#x27;</span>:<br>                <span class="hljs-comment"># attention happens on the input embeddings</span><br>                input_dim = emb_size<br>            <span class="hljs-keyword">elif</span> attn_time == <span class="hljs-string">&#x27;post&#x27;</span>:<br>                <span class="hljs-comment"># attention happens on the output of the rnn</span><br>                input_dim = hsz<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;unsupported attention time&#x27;</span>)<br>            self.attn_combine = nn.Linear(hszXdirs + <span class="hljs-number">2</span> * input_dim, input_dim, bias=<span class="hljs-literal">False</span>)<br><br>            <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;local&#x27;</span>:<br>                <span class="hljs-comment"># local attention over fixed set of output states</span><br>                <span class="hljs-keyword">if</span> attn_length &lt; <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Set attention length to &gt; 0.&#x27;</span>)<br>                self.max_length = attn_length<br>                <span class="hljs-comment"># combines input and previous hidden output layer</span><br>                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=<span class="hljs-literal">False</span>)<br>                <span class="hljs-comment"># combines attention weights with encoder outputs</span><br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;concat&#x27;</span>:<br>                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=<span class="hljs-literal">False</span>)<br>                self.attn_v = nn.Linear(hsz, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;general&#x27;</span>:<br>                <span class="hljs-comment"># equivalent to dot if attn is identity</span><br>                self.attn = nn.Linear(hsz, hszXdirs, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xes, hidden, enc_out, attn_mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;none&#x27;</span>:<br>            <span class="hljs-keyword">return</span> xes<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(hidden) == <span class="hljs-built_in">tuple</span>:<br>            <span class="hljs-comment"># for lstms use the &quot;hidden&quot; state not the cell state</span><br>            hidden = hidden[<span class="hljs-number">0</span>]<br>        last_hidden = hidden[-<span class="hljs-number">1</span>]  <span class="hljs-comment"># select hidden state from last RNN layer</span><br><br>        <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;local&#x27;</span>:<br>            <span class="hljs-keyword">if</span> enc_out.size(<span class="hljs-number">1</span>) &gt; self.max_length:<br>                offset = enc_out.size(<span class="hljs-number">1</span>) - self.max_length<br>                enc_out = enc_out.narrow(<span class="hljs-number">1</span>, offset, self.max_length)<br>            h_merged = torch.cat((xes.squeeze(<span class="hljs-number">1</span>), last_hidden), <span class="hljs-number">1</span>)<br>            attn_weights = F.softmax(self.attn(h_merged), dim=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> attn_weights.size(<span class="hljs-number">1</span>) &gt; enc_out.size(<span class="hljs-number">1</span>):<br>                attn_weights = attn_weights.narrow(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, enc_out.size(<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">else</span>:<br>            hid = last_hidden.unsqueeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;concat&#x27;</span>:<br>                hid = hid.expand(<br>                    last_hidden.size(<span class="hljs-number">0</span>), enc_out.size(<span class="hljs-number">1</span>), last_hidden.size(<span class="hljs-number">1</span>)<br>                )<br>                h_merged = torch.cat((enc_out, hid), <span class="hljs-number">2</span>)<br>                active = torch.tanh(self.attn(h_merged))<br>                attn_w_premask = self.attn_v(active).squeeze(<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;dot&#x27;</span>:<br>                <span class="hljs-keyword">if</span> hid.size(<span class="hljs-number">2</span>) != enc_out.size(<span class="hljs-number">2</span>):<br>                    <span class="hljs-comment"># enc_out has two directions, so double hid</span><br>                    hid = torch.cat([hid, hid], <span class="hljs-number">2</span>)<br>                attn_w_premask = torch.bmm(hid, enc_out.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;general&#x27;</span>:<br>                hid = self.attn(hid)<br>                attn_w_premask = torch.bmm(hid, enc_out.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># calculate activation scores</span><br>            <span class="hljs-keyword">if</span> attn_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># remove activation from NULL symbols</span><br>                attn_w_premask -= (<span class="hljs-number">1</span> - attn_mask) * <span class="hljs-number">1e20</span><br>            attn_weights = F.softmax(attn_w_premask, dim=<span class="hljs-number">1</span>)<br><br>        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="hljs-number">1</span>), enc_out)<br>        merged = torch.cat((xes.squeeze(<span class="hljs-number">1</span>), attn_applied.squeeze(<span class="hljs-number">1</span>)), <span class="hljs-number">1</span>)<br>        output = torch.tanh(self.attn_combine(merged).unsqueeze(<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> output<br><br></code></pre></td></tr></table></figure>
<h3 id="seq2seq-model-2"><a class="markdownIt-Anchor" href="#seq2seq-model-2"></a> Seq2Seq Model</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pad</span>(<span class="hljs-params">tensor, length, dim=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-keyword">if</span> tensor.size(dim) &lt; length:<br>        <span class="hljs-keyword">return</span> torch.cat(<br>            [<br>                tensor,<br>                tensor.new(<br>                    *tensor.size()[:dim],<br>                    length - tensor.size(dim),<br>                    *tensor.size()[dim + <span class="hljs-number">1</span> :],<br>                ).zero_(),<br>            ],<br>            dim=dim,<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> tensor<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2Seq</span>(nn.Module):<br>    RNN_OPTS = &#123;<span class="hljs-string">&#x27;rnn&#x27;</span>: nn.RNN, <span class="hljs-string">&#x27;gru&#x27;</span>: nn.GRU, <span class="hljs-string">&#x27;lstm&#x27;</span>: nn.LSTM&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, args, num_features, padding_idx=<span class="hljs-number">0</span>, start_idx=<span class="hljs-number">1</span>, end_idx=<span class="hljs-number">2</span>, longest_label=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.args = args<br><br>        self.attn_type = args.attn_type<br>        self.NULL_IDX = padding_idx<br>        self.END_IDX = end_idx<br>        self.register_buffer(<span class="hljs-string">&#x27;START&#x27;</span>, torch.LongTensor([start_idx]))<br>        self.longest_label = longest_label<br><br>        rnn_class = Seq2Seq.RNN_OPTS[args.rnn_class]<br>        self.decoder = Decoder(<br>            num_features,<br>            padding_idx=self.NULL_IDX,<br>            rnn_class=rnn_class,<br>            emb_size=args.embedding_size,<br>            hidden_size=args.hidden_size,<br>            num_layers=args.num_layers,<br>            dropout=args.dropout,<br>            attn_type=args.attn_type,<br>            attn_length=args.attn_length,<br>            attn_time=args.attn_time,<br>            bidir_input=args.bidirectional,<br>            numsoftmax=args.num_softmax,<br>            softmax_layer_bias=args.softmax_layer_bias<br>        )<br>        self.encoder = Encoder(<br>            num_features,<br>            padding_idx=self.NULL_IDX,<br>            rnn_class=rnn_class,<br>            emb_size=self.args.embedding_size,<br>            hidden_size=self.args.hidden_size,<br>            num_layers=self.args.num_layers,<br>            dropout=self.args.dropout,<br>            bidirectional=self.args.bidirectional,<br><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xs, ys=<span class="hljs-literal">None</span>, beam_size=<span class="hljs-number">1</span>, topk=<span class="hljs-number">1</span>, prev_enc=<span class="hljs-literal">None</span>, input_lens=<span class="hljs-literal">None</span>, res_lens=<span class="hljs-literal">None</span></span>):<br>        input_xs = xs<br>        nbest_beam_preds, nbest_beam_scores = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>        bsz = <span class="hljs-built_in">len</span>(xs)<br>        <span class="hljs-keyword">if</span> ys <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># keep track of longest label we&#x27;ve ever seen</span><br>            <span class="hljs-comment"># we&#x27;ll never produce longer ones than that during prediction</span><br>            self.longest_label = <span class="hljs-built_in">max</span>(self.longest_label, ys.size(<span class="hljs-number">1</span>))<br><br>        <span class="hljs-keyword">if</span> prev_enc <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            enc_out, hidden, attn_mask = prev_enc<br>        <span class="hljs-keyword">else</span>:<br>            enc_out, hidden = self.encoder(xs, input_lens)<br>            attn_mask = xs.ne(<span class="hljs-number">0</span>).<span class="hljs-built_in">float</span>() <span class="hljs-keyword">if</span> self.attn_type != <span class="hljs-string">&#x27;none&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        encoder_states = (enc_out, hidden, attn_mask)<br>        start = self.START.detach()<br>        starts = start.expand(bsz, <span class="hljs-number">1</span>)<br><br>        predictions = []<br>        scores = []<br>        cand_preds, cand_scores = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> ys <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            y_in = ys.narrow(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, ys.size(<span class="hljs-number">1</span>) - <span class="hljs-number">1</span>)<span class="hljs-comment"># y_in:(bs, tgt_len-1),ys:(bs, tgt_len) </span><br>            xs = torch.cat([starts, y_in], <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> self.attn_type == <span class="hljs-string">&#x27;none&#x27;</span>:<br>                preds, score, hidden = self.decoder(xs, hidden, enc_out, attn_mask)<br>                predictions.append(preds)<br>                scores.append(score)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(ys.size(<span class="hljs-number">1</span>)):<br>                    xi = xs.select(<span class="hljs-number">1</span>, i)<br>                    <span class="hljs-comment"># xi:(bs,), hidden:tuple(h_0:[2,bs,1024], c_0:[2,bs,1024])</span><br>                    <span class="hljs-comment"># 2=D*num_layers, if bi-RNN, D=2, else =1, 1024=H_out,H_cell</span><br>                    <span class="hljs-comment"># enc_out:(bs, src_len, 2048), attn_mask:(bs, src_len)</span><br>                    preds, score, hidden = self.decoder(xi, hidden, enc_out, attn_mask)<br>                    predictions.append(preds)<br>                    scores.append(score)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># here we do search: supported search types: greedy, beam search</span><br>            <span class="hljs-keyword">if</span> beam_size == <span class="hljs-number">1</span>:<br>                done = [<span class="hljs-literal">False</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(bsz)]<br>                total_done = <span class="hljs-number">0</span><br>                xs = starts<br><br>                <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.longest_label):<br>                    <span class="hljs-comment"># generate at most longest_label tokens</span><br>                    preds, score, hidden = self.decoder(<br>                        xs, hidden, enc_out, attn_mask, topk<br>                    )<br>                    scores.append(score)<br>                    xs = preds<br>                    predictions.append(preds)<br><br>                    <span class="hljs-comment"># check if we&#x27;ve produced the end token</span><br>                    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(bsz):<br>                        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> done[b]:<br>                            <span class="hljs-comment"># only add more tokens for examples that aren&#x27;t done</span><br>                            <span class="hljs-keyword">if</span> preds.data[b][<span class="hljs-number">0</span>] == self.END_IDX:<br>                                <span class="hljs-comment"># if we produced END, we&#x27;re done</span><br>                                done[b] = <span class="hljs-literal">True</span><br>                                total_done += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">if</span> total_done == bsz:<br>                        <span class="hljs-comment"># no need to generate any more</span><br>                        <span class="hljs-keyword">break</span><br><br>            <span class="hljs-keyword">elif</span> beam_size &gt; <span class="hljs-number">1</span>:<br>                enc_out, hidden = (<br>                    encoder_states[<span class="hljs-number">0</span>],<br>                    encoder_states[<span class="hljs-number">1</span>],<br>                )  <span class="hljs-comment"># take it from encoder</span><br>                enc_out = enc_out.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>                <span class="hljs-comment"># create batch size num of beams</span><br>                data_device = enc_out.device<br>                beams = [<br>                    Beam(<br>                        beam_size,<br>                        <span class="hljs-number">3</span>,<br>                        <span class="hljs-number">0</span>,<br>                        <span class="hljs-number">1</span>,<br>                        <span class="hljs-number">2</span>,<br>                        min_n_best=beam_size / <span class="hljs-number">2</span>,<br>                        cuda=data_device,<br>                    )<br>                    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(bsz)<br>                ]<br>                <span class="hljs-comment"># init the input with start token</span><br>                xs = starts<br>                <span class="hljs-comment"># repeat tensors to support batched beam</span><br>                xs = xs.repeat(<span class="hljs-number">1</span>, beam_size)<br>                attn_mask = input_xs.ne(<span class="hljs-number">0</span>).<span class="hljs-built_in">float</span>()<br>                attn_mask = attn_mask.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>)<br>                repeated_hidden = []<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(hidden, <span class="hljs-built_in">tuple</span>):<br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(hidden)):<br>                        repeated_hidden.append(<br>                            hidden[i].unsqueeze(<span class="hljs-number">2</span>).repeat(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>)<br>                        )<br>                    hidden = self.unbeamize_hidden(<br>                        <span class="hljs-built_in">tuple</span>(repeated_hidden), beam_size, bsz<br>                    )<br>                <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># GRU</span><br>                    repeated_hidden = hidden.unsqueeze(<span class="hljs-number">2</span>).repeat(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>)<br>                    hidden = self.unbeamize_hidden(repeated_hidden, beam_size, bsz)<br>                enc_out = self.unbeamize_enc_out(enc_out, beam_size, bsz)<br>                xs = xs.view(bsz * beam_size, -<span class="hljs-number">1</span>)<br>                <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.longest_label):<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>((b.done() <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams)):<br>                        <span class="hljs-keyword">break</span><br>                    out = self.decoder(xs, hidden, enc_out)<br>                    scores = out[<span class="hljs-number">1</span>]<br>                    scores = scores.view(bsz, beam_size, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># -1 is a vocab size</span><br>                    <span class="hljs-keyword">for</span> i, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(beams):<br>                        b.advance(F.log_softmax(scores[i, :], dim=-<span class="hljs-number">1</span>))<br>                    xs = torch.cat(<br>                        [b.get_output_from_current_step() <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams]<br>                    ).unsqueeze(-<span class="hljs-number">1</span>)<br>                    permute_hidden_idx = torch.cat(<br>                        [<br>                            beam_size * i + b.get_backtrack_from_current_step()<br>                            <span class="hljs-keyword">for</span> i, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(beams)<br>                        ]<br>                    )<br>                    new_hidden = out[<span class="hljs-number">2</span>]<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(hidden, <span class="hljs-built_in">tuple</span>):<br>                        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(hidden)):<br>                            hidden[i].data.copy_(<br>                                new_hidden[i].data.index_select(<br>                                    dim=<span class="hljs-number">1</span>, index=permute_hidden_idx<br>                                )<br>                            )<br>                    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># GRU</span><br>                        hidden.data.copy_(<br>                            new_hidden.data.index_select(<br>                                dim=<span class="hljs-number">1</span>, index=permute_hidden_idx<br>                            )<br>                        )<br><br>                <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams:<br>                    b.check_finished()<br>                beam_pred = [<br>                    b.get_pretty_hypothesis(b.get_top_hyp()[<span class="hljs-number">0</span>])[<span class="hljs-number">1</span>:] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams<br>                ]<br>                <span class="hljs-comment"># these beam scores are rescored with length penalty!</span><br>                beam_scores = torch.stack([b.get_top_hyp()[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams])<br>                pad_length = <span class="hljs-built_in">max</span>([t.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> beam_pred])<br>                beam_pred = torch.stack(<br>                    [pad(t, length=pad_length, dim=<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> beam_pred], dim=<span class="hljs-number">0</span><br>                )<br><br>                <span class="hljs-comment">#  prepare n best list for each beam</span><br>                n_best_beam_tails = [<br>                    b.get_rescored_finished(n_best=<span class="hljs-built_in">len</span>(b.finished)) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams<br>                ]<br>                nbest_beam_scores = []<br>                nbest_beam_preds = []<br>                <span class="hljs-keyword">for</span> i, beamtails <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(n_best_beam_tails):<br>                    perbeam_preds = []<br>                    perbeam_scores = []<br>                    <span class="hljs-keyword">for</span> tail <span class="hljs-keyword">in</span> beamtails:<br>                        perbeam_preds.append(<br>                            beams[i].get_pretty_hypothesis(<br>                                beams[i].get_hyp_from_finished(tail)<br>                            )<br>                        )<br>                        perbeam_scores.append(tail.score)<br>                    nbest_beam_scores.append(perbeam_scores)<br>                    nbest_beam_preds.append(perbeam_preds)<br><br>                <span class="hljs-keyword">if</span> self.beam_log_freq &gt; <span class="hljs-number">0.0</span>:<br>                    num_dump = <span class="hljs-built_in">round</span>(bsz * self.beam_log_freq)<br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_dump):<br>                        dot_graph = beams[i].get_beam_dot(dictionary=self.<span class="hljs-built_in">dict</span>)<br>                        dot_graph.write_png(<br>                            os.path.join(<br>                                self.beam_dump_path,<br>                                <span class="hljs-string">&quot;&#123;&#125;.png&quot;</span>.<span class="hljs-built_in">format</span>(self.beam_dump_filecnt),<br>                            )<br>                        )<br>                        self.beam_dump_filecnt += <span class="hljs-number">1</span><br><br>                predictions = beam_pred<br>                scores = beam_scores<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(predictions, <span class="hljs-built_in">list</span>):<br>            predictions = torch.cat(predictions, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(scores, <span class="hljs-built_in">list</span>):<br>            scores = torch.cat(scores, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> (<br>            predictions,<br>            scores,<br>            cand_preds,<br>            cand_scores,<br>            encoder_states,<br>            nbest_beam_preds,<br>            nbest_beam_scores,<br>        )<br><br></code></pre></td></tr></table></figure>
<h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1>
<p>[1]Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems 27 (2014).<br>
[2]Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).</p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>seq2seq for personalized dialogue system</div>
      <div>http://example.com/2022/06/12/seq2seq for personalized dialogue system/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>fyxu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年6月12日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/06/12/hmm/" title="隐马尔可夫">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">隐马尔可夫</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>






  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
