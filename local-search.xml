<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>隐马尔可夫</title>
    <link href="/2022/06/12/hmm/"/>
    <url>/2022/06/12/hmm/</url>
    
    <content type="html"><![CDATA[<p>1、根据观测序列找到一个最有可能的隐藏状态序列。<br>2、针对序列标注的问题，输入是一个观测序列，输出是一个标记序列或状态序列。<br>3、HMM的具体应用：中文分词、语音识别、翻译、词性标注<br>4、HMM的定义<br>5、HMM的三要素：状态序列<code>$I=\&#123;i_1,i_2,...,i_T\&#125;$</code>、观察序列<code>$O=\&#123;o_1,o_2,...,o_T\&#125;$</code>、状态集合、观测集合、状态转移矩阵、观测矩阵</p><h1 id="hmmhidden-markov-model"><a class="markdownIt-Anchor" href="#hmmhidden-markov-model"></a> HMM(Hidden Markov Model)</h1><h1 id="一-基本概念"><a class="markdownIt-Anchor" href="#一-基本概念"></a> 一、基本概念</h1><p>HMM是关于时序的概率模型，根据观测序列找到一个最有可能的隐藏状态序列。<strong>HMM描述的是一个由隐藏的马尔科夫链随机生成的不可观测的随机状态序列，再由状态序列生成可观测的随机序列的过程。</strong></p><h1 id="二-hmm的三要素"><a class="markdownIt-Anchor" href="#二-hmm的三要素"></a> 二、HMM的三要素</h1><p>1、HMM由初始的概率分布、状态转移概率分布和观测概率分布确定。<br><code>$Q=\&#123;q_1,q_2,...,q_N\&#125;$</code>：可能的状态集合；<code>$V=\&#123;v_1,v_2,...,v_M\&#125;$</code>：可能的观测集合<br>状态序列<code>$I=\&#123;i_1,i_2,...,i_N\&#125;$</code>；观测序列<code>$O=\&#123;o_1,o_2,...,o_M\&#125;$</code><br><strong>状态转移概率矩阵</strong><code>$A=[a_&#123;ij&#125;]_&#123;N \times N&#125;$</code>；其中<code>$a_&#123;ij&#125;=P(i_&#123;t+1&#125; = q_j \mid i_t = q_i), i=1,2,...,N,j=1,2,...,N$</code>，时刻<code>$t$</code>处于状态<code>$q_i$</code>在时刻<code>$t+1$</code>转移到状态<code>$q_j$</code>的概率<br><strong>观测概率矩阵</strong><code>$B=[b_&#123;j&#125;(k)]_&#123;N \times M&#125;$</code>；其中<code>$b_&#123;j&#125;(k)=p(o_t = v_k \mid i_t = q_j)$</code>，时刻<code>$t$</code>处于状态<code>$q_j$</code>下生成观测<code>$v_k$</code>的概率<br><strong>初始概率矩阵</strong><code>$\pi=\pi_i=P(i_1=q_i),i=1,2...,N$</code>；在时刻<code>$t=1$</code>时，处于状态<code>$q_i$</code>的概率<br>2、隐马模型<code>$\lambda = (A,B,\pi)$</code>，<code>$A$</code>和<code>$\pi$</code>确定隐藏的马尔科夫链，生成不可观测的状态序列；<code>$B$</code>将状态序列转换成观测序列</p><h2 id="基本假设"><a class="markdownIt-Anchor" href="#基本假设"></a> 基本假设</h2><p>1、齐次马尔科夫性假设：假设隐藏的马尔科夫链在任意时刻<code>$t$</code>的状态只依赖于前一时刻的状态，与其他时刻和状态无关，即：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs math">P(i_t \mid i_&#123;t-1&#125;, o_&#123;t-1&#125;, ..., i_&#123;1&#125;, o_&#123;1&#125;), t=1,2,...,T<br></code></pre></td></tr></table></figure><p>2、观测独立性假设：假设任意时刻的观测只依赖于该时刻的马尔科夫链状态，与其他观测和状态无关<br><img src="/jpg/hmm.png" alt="hmm.png"></p><h1 id="三-例子"><a class="markdownIt-Anchor" href="#三-例子"></a> 三、例子</h1><p>假设有四个盒子，每个盒子情况如下：</p><table><thead><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>红球</td><td>5</td><td>3</td><td>6</td><td>8</td></tr><tr><td>白球</td><td>5</td><td>7</td><td>4</td><td>2</td></tr></tbody></table><p>按照下面的方法抽球，产生一个球的颜色观测序列：<br>步骤：从 4 个盒子里以等概率随机选取 1 个盒子，<strong>规则如下</strong>：</p><ul><li>如果当前盒子是盒子 1，那么下一个盒子一定是盒子 2；</li><li>如果当前是盒子 2 或 3，那么分别以概率 0.4 和 0.6 转移到左边或右边的盒子；</li><li>如果当前是盒子 4，那么各以 0.5 的概率停留在 4 或转移到盒子 3；</li><li>确定转移的盒子后，再从这个盒子里随机抽一个球，记录其颜色，放回；</li><li>如此下去，重复进行 5 次，得到一个球的颜色的观测序列：<code>$O = (红、红、白、白、红)$</code><br>在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取出的，也就是观测不到盒子的序列。<strong>盒子的序列是隐藏序列，颜色的序列是观测序列。</strong><br>状态的集合<code>$Q=\&#123;盒子_1,盒子_2,...,盒子_N\&#125;, N=4$</code>，观测的集合<code>$V=\&#123;红,白\&#125;$</code>，序列长度和观测长度<code>$T=5$</code><br>初始概率分布：<code>$\pi = \begin&#123;bmatrix&#125;0.25 \\ 0.25 \\ 0.25 \\ 0.25 \end&#123;bmatrix&#125;$</code>，随机从四个盒子中挑选一个盒子<br>状态转移概率分布：<code>$A=\begin&#123;bmatrix&#125;0 &amp; 1 &amp; 0 &amp; 0 \\ 0.4 &amp; 0 &amp; 0.6 &amp; 0 \\ 0 &amp; 0.4 &amp; 0 &amp; 0.6 \\ 0 &amp; 0 &amp; 0.5 &amp; 0.5 \end&#123;bmatrix&#125;$</code>，对应上述的四个规则<br>观测概率分布：<code>$B=\begin&#123;bmatrix&#125;0.5 &amp; 0.5 \\ 0.3 &amp; 0.7 \\ 0.6 &amp; 0.4 \\0.8 &amp; 0.2 \end&#123;bmatrix&#125;$</code>，对应每个盒子中红球和白球各自的比例</li></ul><h2 id="观测序列的生成过程即oo_1o_2o_t的生成过程"><a class="markdownIt-Anchor" href="#观测序列的生成过程即oo_1o_2o_t的生成过程"></a> 观测序列的生成过程，即<code>$O=(o_1,o_2,...,o_T)$</code>的生成过程</h2><p>按照初始状态分布 <code>$\pi$</code> 产生状态 <code>$i_1$</code>，令<code>$t=1$</code>;<br>根据<code>$i_1$</code>的观测概率分布<code>$b_&#123;i_&#123;1&#125;&#125;(k)$</code>生成<code>$o_1$</code>；<br>按照状态<code>$i_1$</code>的状态转移概率分布<code>$[a_&#123;i_1 i_&#123;2&#125;&#125;]$</code>产生状态<code>$i_&#123;2&#125;$</code><br>…<br>根据<code>$i_t$</code>的观测概率分布<code>$b_&#123;i_&#123;t&#125;&#125;(k)$</code>生成<code>$o_t$</code>；<br>按照状态<code>$i_t$</code>的状态转移概率分布<code>$[a_&#123;i_t i_&#123;t+1&#125;&#125;]$</code>产生状态<code>$i_&#123;t+1&#125;, i_&#123;t+1&#125;=1,2,...,N$</code></p><h2 id="q1计算观测序列概率po-mid-lambda"><a class="markdownIt-Anchor" href="#q1计算观测序列概率po-mid-lambda"></a> Q1：计算观测序列概率<code>$P(O \mid \lambda)$</code></h2><h3 id="前向算法"><a class="markdownIt-Anchor" href="#前向算法"></a> 前向算法</h3><p>输入<code>$\lambda, O$</code>；输出<code>$P\left(O \mid \lambda \right)$</code><br>定义前向概率<code>$\alpha_t (i)=P\left(o_1, o_2,...,o_t, i_t=q_i \mid \lambda \right)$</code><br>(1)初始化<code>$\alpha_1(i)=\pi_i b_i(o_1), i=1,2,...,N$</code>,先选盒子再取球，<code>$\pi_i$</code>为取每个盒子的概率，<code>$b_i(o_1)$</code>为从选出的盒子中取到观察为<code>$o_1$</code>的概率<br>(2)递推<code>$\alpha_2(i)=\left[\sum_&#123;j=1&#125;^N \alpha_1(i)a_&#123;ji&#125;\right]b_i(o_&#123;2&#125;), i=1,2,...,N$</code><br>…<br><code>$\alpha_3(i)=\left[\sum_&#123;j=1&#125;^N \alpha_2(i)a_&#123;ji&#125;\right]b_i(o_&#123;3&#125;), i=1,2,...,N$</code><br>对于<code>$t=1,2,...,T-1$</code>， <code>$\alpha_&#123;t+1&#125;(i)=\left[\sum_&#123;j=1&#125;^N \alpha_t(i)a_&#123;ji&#125;\right]b_i(o_&#123;t+1&#125;), i=1,2,...,N$</code>，其中<code>$\alpha_&#123;t+1&#125;(i)$</code>为第<code>$t+1$</code>个时间步，选中第<code>$i$</code>个盒子且取出<code>$o_&#123;t+1&#125;$</code>的概率；<code>$\left[\sum_&#123;j=1&#125;^N \alpha_t(i)a_&#123;ji&#125;\right]$</code>为<code>$t$</code>时刻，N个状态求和，最终得到<code>$o_1, o_2,...,o_t$</code>，并在<code>$t+1$</code>时刻处于<code>$q_i$</code>的联合概率<br>(3)终止<code>$P(O \mid \lambda)=\sum_&#123;i=1&#125;^N \alpha_T(i)$</code>，<code>$\alpha_T(i)=P\left(o_1,o_2,...,o_T \mid \lambda \right)$</code></p><h3 id="后向算法"><a class="markdownIt-Anchor" href="#后向算法"></a> 后向算法</h3><p>输入<code>$\lambda, O$</code>；输出<code>$P\left(O \mid \lambda \right)$</code><br>定义后项概率<code>$\beta_t (i)=P\left(o_&#123;t+1&#125;,o_&#123;t+2&#125;,...,o_&#123;T&#125; \mid i_t=q_i, \lambda \right)$</code>，在时刻<code>$t$</code>状态为<code>$q_i$</code>的条件下，从<code>$t+1$</code>到<code>$T$</code>的部分观测序列为<code>$o_&#123;t+1&#125;, o_&#123;t+2&#125;,...,o_T$</code>的概率。<br>(1)初始化<code>$\beta_T (i)=1, i=1,2,...,N$</code>，对最终时刻的所有状态<code>$q_i$</code>规定<code>$\beta_T (i)=1$</code><br>(2)递推<code>$\beta_&#123;T-1&#125; (i)=\sum_&#123;j=1&#125;^N a_&#123;ij&#125;b_&#123;j&#125;(o_T) \beta_T (j), i=1,2,...,N$</code><br><code>$\beta_&#123;T-2&#125; (i)=\sum_&#123;j=1&#125;^N a_&#123;ij&#125;b_&#123;j&#125;(o_&#123;T-1&#125;) \beta_&#123;T-1&#125; (j), i=1,2,...,N$</code><br>…<br>对<code>$t=T-1,T-2,...,1$</code>,<code>$\beta_&#123;t&#125; (i)=\sum_&#123;j=1&#125;^N a_&#123;ij&#125;b_&#123;j&#125;(o_&#123;t+1&#125;) \beta_&#123;t+1&#125; (j), i=1,2,...,N$</code><br>(3)终止，计算<code>$P\left(O \mid \lambda \right)=\sum_&#123;i=1&#125;^N \pi_i b_i(o_1) \beta_1(i)$</code><br><strong>结合上述的前向和后项算法，将观测序列的概率定义为：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs math">P(O \mid \lambda) = \sum_&#123;i=1&#125;^N \sum_&#123;j=1&#125;^N \alpha_t(i) a_&#123;ij&#125;b_j(o_&#123;t+1&#125;) \beta_&#123;t+1&#125;(j), t=1,2,...,T-1<br></code></pre></td></tr></table></figure><h2 id="概率与期望的计算"><a class="markdownIt-Anchor" href="#概率与期望的计算"></a> 概率与期望的计算</h2><p>1、定义在<code>$t$</code>时刻处于状态<code>$q_i$</code>的概率，记作<code>$\gamma_t (i)=P\left(i_t=q_i \mid O, \lambda \right)$</code>，通过条件概率公式得</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs math">\begin&#123;aligned&#125;<br>\gamma_t (i) &amp;= P\left(i_t=q_i \mid O, \lambda \right) \\<br>             &amp;= \frac&#123;P(i_t=q_i, O, \lambda)&#125;&#123;P(O, \lambda)&#125; \\<br>             &amp;= \frac&#123;P(\lambda)P(i_t=q_i, O \mid \lambda)&#125;&#123;P(\lambda)P(O \mid \lambda)&#125; \\<br>             &amp;= \frac&#123;P(i_t=q_i, O \mid \lambda)&#125;&#123;P(O \mid \lambda)&#125;<br>\end&#123;aligned&#125;<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs math">\alpha_t (i)=P\left(o_1, o_2,...,o_t, i_t=q_i \mid \lambda \right) \\<br>\beta_t (i)=P\left(o_&#123;t+1&#125;,o_&#123;t+2&#125;,...,o_&#123;T&#125; \mid i_t=q_i, \lambda \right) \\<br>\alpha_t (i) \beta_t (i) = P(i_t=q_i, O \mid \lambda)<br></code></pre></td></tr></table></figure><p>由上所得<code>$\gamma_t (i)=\frac&#123;\alpha_t (i) \beta_t (i)&#125;&#123;\sum_&#123;j=1&#125;^N \alpha_t (j) \beta_t (j)&#125;$</code><br>2、给定模型<code>$\lambda$</code>和观测<code>$O$</code>，在时刻<code>$t$</code>处于状态<code>$q_i$</code>且在时刻<code>$t=1$</code>处于状态<code>$q_j$</code>的概率记作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs math">\xi_t (i,j)=P\left(i_t = q_i, i_&#123;t+1&#125; = q_j \mid O, \lambda\right)<br></code></pre></td></tr></table></figure><p>通过前向后向概率计算得</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs math">\begin&#123;aligned&#125;<br>\xi_t (i,j) &amp;= \frac&#123;P\left(i_t=q_i, i_&#123;t+1&#125;=q_j \mid O, \lambda\right)&#125;&#123;P\left(O | \lambda\right)&#125; \\<br>            &amp;= \frac&#123;P\left(i_t=q_i, i_&#123;t+1&#125;=q_j \mid O, \lambda\right)&#125;&#123;\sum_&#123;i=1&#125;^N \sum_&#123;j=1&#125;^N P\left(i_t=q_i, i_&#123;t+1&#125;=q_j, O \mid \lambda\right)&#125; <br>\end&#123;aligned&#125;<br></code></pre></td></tr></table></figure><p>3、将<code>$\gamma_t (i)$</code>和<code>$\xi_t (i,j)$</code>对各个时刻<code>$t$</code>求和，可以得到一些有用的期望值：</p><ul><li>在观测<code>$O$</code>下状态<code>$i$</code>出现的期望值<code>$\sum_&#123;t=1&#125;^T \gamma_t (i)$</code></li><li>在观测<code>$O$</code>下由状态<code>$i$</code>转移的期望值<code>$\sum_&#123;t=1&#125;^&#123;T-1&#125; \gamma_t (i)$</code></li><li>在观测<code>$O$</code>下状态<code>$i$</code>转移到状态<code>$j$</code>的期望值<code>$\sum_&#123;t=1&#125;^&#123;T-1&#125; \xi_t (i,j)$</code></li></ul><h2 id="hmm参数学习的方法"><a class="markdownIt-Anchor" href="#hmm参数学习的方法"></a> HMM参数学习的方法</h2><h3 id="监督学习的方法"><a class="markdownIt-Anchor" href="#监督学习的方法"></a> 监督学习的方法</h3><p>假设已给训练数据包含<code>$S$</code>个长度相同的观测序列和对应的状态序列<code>$\&#123;(O_1,I_1),(O_2,I_2),...,(O_S,I_S)\&#125;$</code>，可以利用极大似然估计来估计隐马模型参数。</p><h3 id="无监督学习的方法baum-welch-算法"><a class="markdownIt-Anchor" href="#无监督学习的方法baum-welch-算法"></a> 无监督学习的方法（Baum-Welch 算法）</h3><h2 id="hmm预测算法"><a class="markdownIt-Anchor" href="#hmm预测算法"></a> HMM预测算法</h2><h3 id="近似算法"><a class="markdownIt-Anchor" href="#近似算法"></a> 近似算法</h3><h3 id="维特比算法"><a class="markdownIt-Anchor" href="#维特比算法"></a> 维特比算法</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>seq2seq for personalized dialogue system</title>
    <link href="/2022/06/12/seq2seq%20for%20personalized%20dialogue%20system/"/>
    <url>/2022/06/12/seq2seq%20for%20personalized%20dialogue%20system/</url>
    
    <content type="html"><![CDATA[<h1 id="seq2seq-model"><a class="markdownIt-Anchor" href="#seq2seq-model"></a> Seq2Seq Model</h1><h2 id="seq2seq结构"><a class="markdownIt-Anchor" href="#seq2seq结构"></a> Seq2Seq结构</h2><p>Seq2Seq(Sequence-to-Sequence)[1,2]模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。Seq2Seq模型包含编码器(Encoder)和解码器(Decoder)两部分，它们通常是两个不同的 RNN。<br><img src="/jpg/s2s.png" alt="s2s.png"></p><h2 id="seq2seq的训练"><a class="markdownIt-Anchor" href="#seq2seq的训练"></a> Seq2Seq的训练</h2><h3 id="1-在seq2seq的训练过程中编码和解码时同时训练的编码的输入和解码的输入作为统一的整体输入进模型解码的输出作为整体的输出"><a class="markdownIt-Anchor" href="#1-在seq2seq的训练过程中编码和解码时同时训练的编码的输入和解码的输入作为统一的整体输入进模型解码的输出作为整体的输出"></a> 1、在Seq2Seq的训练过程中，编码和解码时同时训练的；编码的输入和解码的输入作为统一的整体输入进模型，解码的输出作为整体的输出。</h3><h3 id="2-rnn的训练更新有两种模式"><a class="markdownIt-Anchor" href="#2-rnn的训练更新有两种模式"></a> 2、RNN的训练更新有两种模式</h3><ul><li>free-running mode: 上一个state的输出作为下一个state的输入</li><li><strong>teacher-forcing mode</strong>: 使用来自先验时间步长的输出作为输入<br>free-running mode存在的问题(1)收敛慢(2)模型不稳定(3)Poor skil；训练迭代过程早期的RNN预测能力非常弱，几乎不能给出好的生成结果。如果某一个unit产生了垃圾结果，必然会影响后面一片unit的学习。错误结果会导致后续的学习都受到不好的影响，导致学习速度变慢，难以收敛。因此出现了teacher forcing。<br>teacher-forcing的问题: <strong>Exposure Bias</strong>，使用teacher-forcing，在训练过程中，模型会有较好的效果，但是在测试的时候因为不能得到ground truth的支持，存在训练测试偏差，模型会变得脆弱。</li></ul><h1 id="personalized-dialogue-system"><a class="markdownIt-Anchor" href="#personalized-dialogue-system"></a> Personalized Dialogue System</h1><h2 id="训练参数的设置设置随机种子"><a class="markdownIt-Anchor" href="#训练参数的设置设置随机种子"></a> 训练参数的设置&amp;设置随机种子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_set_args</span>():<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--sed&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">1234</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;The random seed.&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--dataset_path&#x27;</span>, default=<span class="hljs-string">&#x27;&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--cache&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&#x27;./dataset_cache/&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--tokenize&#x27;</span>, default=<span class="hljs-string">&#x27;split&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--train_bs&#x27;</span>, default=<span class="hljs-number">64</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--valid_bs&#x27;</span>, default=<span class="hljs-number">32</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--epochs&#x27;</span>, default=<span class="hljs-number">400</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--device&#x27;</span>, default=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--attn_type&#x27;</span>, default=<span class="hljs-string">&#x27;general&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--rnn_class&#x27;</span>, default=<span class="hljs-string">&#x27;lstm&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--lr&#x27;</span>, default=<span class="hljs-number">3</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--momentum&#x27;</span>, default=<span class="hljs-number">0.9</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--clip&#x27;</span>, default=<span class="hljs-number">0.1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--save_path&#x27;</span>, default=<span class="hljs-string">&#x27;./ckp&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--load_from_path&#x27;</span>, default=<span class="hljs-string">&#x27;./ckp/v1/199_model.pth&#x27;</span>)<br><br>    parser.add_argument(<span class="hljs-string">&#x27;--embedding_size&#x27;</span>, default=<span class="hljs-number">256</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--hidden_size&#x27;</span>, default=<span class="hljs-number">1024</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--attn_length&#x27;</span>, default=<span class="hljs-number">48</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_layers&#x27;</span>, default=<span class="hljs-number">2</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--dropout&#x27;</span>, default=<span class="hljs-number">0.1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--attn_time&#x27;</span>, default=<span class="hljs-string">&#x27;post&#x27;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--bidirectional&#x27;</span>, default=<span class="hljs-literal">True</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--softmax_layer_bias&#x27;</span>, default=<span class="hljs-literal">False</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_softmax&#x27;</span>, default=<span class="hljs-number">1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--beam_size&#x27;</span>, default=<span class="hljs-number">1</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--topk&#x27;</span>, default=<span class="hljs-number">1</span>)<br><br>    args = parser.parse_args()<br>    <span class="hljs-keyword">return</span> args<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_set_seed</span>(<span class="hljs-params">args</span>):<br>    np.random.seed(args.sed)<br>    torch.manual_seed(args.sed)<br>    torch.cuda.manual_seed_all(args.sed)<br>    np.random.seed(args.sed)<br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><h2 id="获取数据集构建glove所需要的词典"><a class="markdownIt-Anchor" href="#获取数据集构建glove所需要的词典"></a> 获取数据集，构建Glove所需要的词典</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><code class="hljs python">PERSONACHAT_URL = <span class="hljs-string">&quot;https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json&quot;</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DialogDict</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, corpus_path, args</span>):<br>        self.corpus_path = corpus_path<br>        self.ind2tok = &#123;&#125;<br>        self.tok2ind = &#123;&#125;<br>        self.freq = defaultdict(<span class="hljs-built_in">int</span>)<br>        self.__null__ = <span class="hljs-string">&#x27;__null__&#x27;</span><br>        self.__start__ = <span class="hljs-string">&#x27;__start__&#x27;</span><br>        self.__end__ = <span class="hljs-string">&#x27;__end__&#x27;</span><br>        self.__unk__ = <span class="hljs-string">&#x27;__unk__&#x27;</span><br>        self.args = args<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">escape</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-string">r&quot;&quot;&quot;Replace potential special characters with escaped version.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For example, \n =&gt; \\n and \t =&gt; \\t</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param s: string to escape</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> s.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;\\n&#x27;</span>).replace(<span class="hljs-string">&#x27;\t&#x27;</span>, <span class="hljs-string">&#x27;\\t&#x27;</span>).replace(<span class="hljs-string">&#x27;\r&#x27;</span>, <span class="hljs-string">&#x27;\\r&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unescape</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-string">r&quot;&quot;&quot;Revert escaped characters back to their special version.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For example, \\n =&gt; \n and \\t =&gt; \t</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param s: string to unescape</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> s.replace(<span class="hljs-string">&#x27;\\n&#x27;</span>, <span class="hljs-string">&#x27;\n&#x27;</span>).replace(<span class="hljs-string">&#x27;\\t&#x27;</span>, <span class="hljs-string">&#x27;\t&#x27;</span>).replace(<span class="hljs-string">&#x27;\\r&#x27;</span>, <span class="hljs-string">&#x27;\r&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tokenize</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-keyword">return</span> text.replace(<span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27; . &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27; , &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;;&#x27;</span>, <span class="hljs-string">&#x27; ; &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;:&#x27;</span>, <span class="hljs-string">&#x27; : &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;!&#x27;</span>, <span class="hljs-string">&#x27; ! &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;?&#x27;</span>, <span class="hljs-string">&#x27; ? &#x27;</span>) \<br>               .replace(<span class="hljs-string">&#x27;:&#x27;</span>, <span class="hljs-string">&#x27; : &#x27;</span>) \<br>               .split()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add_token</span>(<span class="hljs-params">self, word</span>):<br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.tok2ind:<br>            index = <span class="hljs-built_in">len</span>(self.tok2ind)<br>            self.tok2ind[word] = index<br>            self.ind2tok[index] = word<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add_to_dict</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens:<br>            self.add_token(token)<br>            self.freq[token] += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self</span>):<br>        self.tok2ind.update(&#123;<span class="hljs-string">&#x27;__null__&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;__start__&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;__end__&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;__unk__&#x27;</span>: <span class="hljs-number">3</span>&#125;)<br>        self.ind2tok.update(&#123;<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;__null__&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;__start__&#x27;</span>, <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;__end__&#x27;</span>, <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;__unk__&#x27;</span>&#125;)<br>        self.freq[self.__null__] = <span class="hljs-number">1000000003</span><br>        self.freq[self.__start__] = <span class="hljs-number">1000000002</span><br>        self.freq[self.__end__] = <span class="hljs-number">1000000001</span><br>        self.freq[self.__unk__] = <span class="hljs-number">1000000000</span><br><br>        <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(self.corpus_path, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;utf8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>                self.add_to_dict(self._tokenize(line))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save</span>(<span class="hljs-params">self</span>):<br>        file_name = self.args.cache + <span class="hljs-string">&#x27;Dialogue.dict&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Dictionary: saving dictionary to &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(file_name))<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&#x27;a&#x27;</span>) <span class="hljs-keyword">as</span> write:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.ind2tok)):<br>                tok = self.ind2tok[i]<br>                cnt = self.freq[tok]<br>                write.write(<span class="hljs-string">&#x27;&#123;tok&#125;\t&#123;cnt&#125;\n&#x27;</span>.<span class="hljs-built_in">format</span>(tok=self.escape(tok), cnt=cnt))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">self</span>):<br>        file_name = self.args.cache + <span class="hljs-string">&#x27;Dialogue.dict&#x27;</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Dictionary: loading dictionary from &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(file_name))<br>        lower_special = self.__null__ == self.__null__.lower()<br>        SPECIAL_TOKENS = &#123;<span class="hljs-string">&#x27;__unk__&#x27;</span>, <span class="hljs-string">&#x27;__null__&#x27;</span>, <span class="hljs-string">&#x27;__end__&#x27;</span>, <span class="hljs-string">&#x27;__start__&#x27;</span>&#125;<br>        <span class="hljs-keyword">with</span> codecs.<span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>, errors=<span class="hljs-string">&#x27;ignore&#x27;</span>) <span class="hljs-keyword">as</span> read:<br>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> read:<br>                split = line.strip().split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>                token = self.unescape(split[<span class="hljs-number">0</span>])<br>                <span class="hljs-keyword">if</span> lower_special <span class="hljs-keyword">and</span> token <span class="hljs-keyword">in</span> SPECIAL_TOKENS:<br>                    token = token.lower()<br>                cnt = <span class="hljs-built_in">int</span>(split[<span class="hljs-number">1</span>]) <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>                self.freq[token] = cnt<br>                self.add_token(token)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[ num words =  %d ]&#x27;</span> % <span class="hljs-built_in">len</span>(self.ind2tok))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_decode</span>(<span class="hljs-params">self, tokens_list</span>):<br>        <span class="hljs-keyword">return</span> [self.ind2tok[w.item()] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens_list]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_dataset</span>(<span class="hljs-params">args</span>):<br>    corpus_path = args.cache + <span class="hljs-string">&#x27;corpus.txt&#x27;</span><br>    datasetpath = args.cache + <span class="hljs-string">&#x27;dataset.json&#x27;</span><br>    dic_path = args.cache + <span class="hljs-string">&#x27;Dialogue.dict&#x27;</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(corpus_path) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> os.path.exists(datasetpath) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> os.path.exists(dic_path):<br>        dataset_path = args.dataset_path <span class="hljs-keyword">or</span> PERSONACHAT_URL<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Download dataset from %s&quot;</span>, dataset_path)<br>        personachat_file = cached_path(dataset_path)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(personachat_file, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>            dataset = json.loads(f.read())<br>        corpus = codecs.<span class="hljs-built_in">open</span>(corpus_path, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;utf8&#x27;</span>)<br>        datasets = &#123;<span class="hljs-string">&quot;train&quot;</span>: [], <span class="hljs-string">&quot;valid&quot;</span>: []&#125;<br>        <span class="hljs-keyword">for</span> dataset_name, data <span class="hljs-keyword">in</span> dataset.items():<br>            <span class="hljs-keyword">for</span> dialog <span class="hljs-keyword">in</span> tqdm.tqdm(data, desc=<span class="hljs-string">&quot;Process Data&quot;</span>):<br>                persona = dialog[<span class="hljs-string">&#x27;personality&#x27;</span>].copy()<br>                <span class="hljs-keyword">for</span> utter <span class="hljs-keyword">in</span> dialog[<span class="hljs-string">&#x27;utterances&#x27;</span>]:<br>                    query = utter[<span class="hljs-string">&#x27;history&#x27;</span>][-<span class="hljs-number">1</span>]<br>                    datasets[dataset_name].append((<span class="hljs-string">&#x27;persona : &#x27;</span> + <span class="hljs-string">&#x27; &#x27;</span>.join(persona), query, utter[<span class="hljs-string">&#x27;candidates&#x27;</span>][-<span class="hljs-number">1</span>]))<br>                    corpus.write(<span class="hljs-string">&#x27;persona : &#x27;</span> + <span class="hljs-string">&#x27; &#x27;</span>.join(persona) + <span class="hljs-string">&#x27; &#x27;</span> + query + <span class="hljs-string">&#x27; &#x27;</span> + utter[<span class="hljs-string">&#x27;candidates&#x27;</span>][-<span class="hljs-number">1</span>] + <span class="hljs-string">&#x27;\n&#x27;</span>)<br>        corpus.close()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Save Datasets...&quot;</span>)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(datasetpath, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            json.dump(datasets, f)<br>        dialog_dict = DialogDict(corpus_path, args)<br>        dialog_dict.build()<br>        dialog_dict.save()<br>    <span class="hljs-keyword">else</span>:<br>        dialog_dict = DialogDict(corpus_path, args)<br>        dialog_dict.load()<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(datasetpath, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            datasets = json.load(f)<br><br>    <span class="hljs-keyword">return</span> datasets, dialog_dict<br><br></code></pre></td></tr></table></figure><h2 id="构造dataset和dataloader"><a class="markdownIt-Anchor" href="#构造dataset和dataloader"></a> 构造Dataset和Dataloader</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvAI2</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dataset</span>):<br>        self.dataset = dataset<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.dataset[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tokenized</span>(<span class="hljs-params">self, seq, dialog_dict</span>):<br>        ret = []<br>        <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq.split():<br>            <span class="hljs-keyword">try</span>:<br>                ret.append(dialog_dict.tok2ind[w])<br>            <span class="hljs-keyword">except</span>:<br>                ret.append(dialog_dict.tok2ind[<span class="hljs-string">&#x27;__unk__&#x27;</span>])<br>        <span class="hljs-keyword">return</span> ret<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">self, batch, dialog_dict</span>):<br>        input_seq = [dialog_dict.__start__ + <span class="hljs-string">&#x27; &#x27;</span> + b[<span class="hljs-number">0</span>] + <span class="hljs-string">&#x27; &#x27;</span> + b[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> batch]<br>        response_seq = [b[<span class="hljs-number">2</span>] + <span class="hljs-string">&#x27; &#x27;</span> + dialog_dict.__end__ <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> batch]<br><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(input_seq) == <span class="hljs-built_in">len</span>(response_seq)<br>        input_seq_tokenized = [self._tokenized(seq, dialog_dict) <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> input_seq]<br>        response_seq_tokenized = [self._tokenized(res, dialog_dict) <span class="hljs-keyword">for</span> res <span class="hljs-keyword">in</span> response_seq]<br><br>        sort_map = [(i, j) <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(input_seq_tokenized, response_seq_tokenized)]<br>        sorted_map = <span class="hljs-built_in">sorted</span>(sort_map, key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-number">0</span>]), reverse=<span class="hljs-literal">True</span>)<br><br>        input_lens = [<span class="hljs-built_in">len</span>(it[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map]<br>        res_lens = [<span class="hljs-built_in">len</span>(it[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map]<br><br>        <span class="hljs-built_in">input</span> = pad_sequence([torch.tensor(it[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map], batch_first=<span class="hljs-literal">True</span>, padding_value=dialog_dict.tok2ind[dialog_dict.__null__])<br>        res = pad_sequence([torch.tensor(it[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> sorted_map], batch_first=<span class="hljs-literal">True</span>, padding_value=dialog_dict.tok2ind[dialog_dict.__null__])<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span>, res, input_lens, res_lens<br>train_dataset, valid_dataset = ConvAI2(datasets[<span class="hljs-string">&#x27;train&#x27;</span>]), ConvAI2(datasets[<span class="hljs-string">&#x27;valid&#x27;</span>])<br>train_loader, valid_loader = DataLoader(dataset=train_dataset, batch_size=args.train_bs, shuffle=<span class="hljs-literal">True</span>, collate_fn=<span class="hljs-keyword">lambda</span> x: train_dataset.collate_fn(x, dialog_dict)), \<br>                                 DataLoader(dataset=valid_dataset[:<span class="hljs-number">7021</span>], batch_size=args.valid_bs, shuffle=<span class="hljs-literal">False</span>, collate_fn=<span class="hljs-keyword">lambda</span> x: train_dataset.collate_fn(x, dialog_dict))<br></code></pre></td></tr></table></figure><h2 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h2><h3 id="encoder"><a class="markdownIt-Anchor" href="#encoder"></a> Encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            num_features,</span><br><span class="hljs-params">            padding_idx=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">            rnn_class=<span class="hljs-string">&#x27;lstm&#x27;</span>,</span><br><span class="hljs-params">            emb_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            hidden_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            num_layers=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">            dropout=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">            bidirectional=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            sparse=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dropout = nn.Dropout(p=dropout)<br>        self.layers = num_layers<br>        self.bidirectional = bidirectional<br>        self.hidden_size = hidden_size<br>        self.emb_layer = nn.Embedding(<br>            num_features,<br>            emb_size,<br>            padding_idx=padding_idx,<br>            sparse=sparse)<br><br>        self.rnn = rnn_class(<br>            emb_size,<br>            hidden_size,<br>            num_layers,<br>            dropout=dropout,<br>            batch_first=<span class="hljs-literal">True</span>,<br>            bidirectional=bidirectional<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xs, input_lens=<span class="hljs-literal">None</span></span>):<br>        bsz = <span class="hljs-built_in">len</span>(xs)<br><br>        <span class="hljs-comment"># embed input tokens</span><br>        xes = self.dropout(self.emb_layer(xs))<br>        xes = pack_padded_sequence(xes, input_lens, batch_first=<span class="hljs-literal">True</span>)<br>        encoder_output, hidden = self.rnn(xes)<br>        encoder_output, _ = pad_packed_sequence(encoder_output, batch_first=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        if self.bidirectional:</span><br><span class="hljs-string">            if isinstance(self.rnn, nn.LSTM):</span><br><span class="hljs-string">                hidden = (</span><br><span class="hljs-string">                    hidden[0].view(-1, 2, bsz, self.hidden_size).max(1)[0],</span><br><span class="hljs-string">                    hidden[1].view(-1, 2, bsz, self.hidden_size).max(1)[0],</span><br><span class="hljs-string">                )# 2, bidirectional</span><br><span class="hljs-string">            else:</span><br><span class="hljs-string">                hidden = hidden.view(-1, 2, bsz, self.hidden_size).max(1)[0]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-keyword">return</span> encoder_output, hidden<br><br></code></pre></td></tr></table></figure><h3 id="decoder"><a class="markdownIt-Anchor" href="#decoder"></a> Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            num_features,</span><br><span class="hljs-params">            padding_idx=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">            rnn_class=<span class="hljs-string">&#x27;lstm&#x27;</span>,</span><br><span class="hljs-params">            emb_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            hidden_size=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">            num_layers=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">            dropout=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">            bidir_input=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            sparse=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            numsoftmax=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">            softmax_layer_bias=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            attn_type=<span class="hljs-string">&#x27;none&#x27;</span>,</span><br><span class="hljs-params">            attn_length=-<span class="hljs-number">1</span>,</span><br><span class="hljs-params">            attn_time=<span class="hljs-string">&#x27;pre&#x27;</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dropout = nn.Dropout(p=dropout)<br>        self.layers = num_layers<br>        self.hsz = hidden_size<br>        self.esz = emb_size<br><br>        self.embedd_layer = nn.Embedding(num_features, emb_size, padding_idx=padding_idx, sparse=sparse)<br>        self.rnn = rnn_class(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=<span class="hljs-literal">True</span>, bidirectional=bidir_input)<br><br>        <span class="hljs-comment"># rnn output to embedding</span><br>        <span class="hljs-keyword">if</span> hidden_size != emb_size <span class="hljs-keyword">and</span> numsoftmax == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># self.o2e = RandomProjection(hidden_size, emb_size)</span><br>            <span class="hljs-comment"># other option here is to learn these weights</span><br>            self.o2e = nn.Linear(hidden_size, emb_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># no need for any transformation here</span><br>            self.o2e = <span class="hljs-keyword">lambda</span> x: x<br>        <span class="hljs-comment"># embedding to scores, use custom linear to possibly share weights</span><br>        self.e2s = nn.Linear(emb_size, num_features, bias=softmax_layer_bias)<br><br>        self.attn_type = attn_type<br>        self.attn_time = attn_time<br>        self.attention = AttentionLayer(<br>            attn_type=attn_type,<br>            hidden_size=hidden_size,<br>            emb_size=emb_size,<br>            bidirectional=bidir_input,<br>            attn_length=attn_length,<br>            attn_time=attn_time,<br>        )<br><br>        self.numsoftmax = numsoftmax<br>        <span class="hljs-keyword">if</span> numsoftmax &gt; <span class="hljs-number">1</span>:<br>            self.sofrmax = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>            self.prior = nn.Linear(hidden_size, numsoftmax, bias=<span class="hljs-literal">False</span>)<br>            self.latent = nn.Linear(hidden_size, numsoftmax * emb_size)<br>            self.activation = nn.Tanh()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xs, hidden, encoder_output, attn_mask=<span class="hljs-literal">None</span>, topk=<span class="hljs-number">1</span></span>):<br>        xes = self.dropout(self.embedd_layer(xs))<br>        <span class="hljs-keyword">if</span> self.attn_time == <span class="hljs-string">&#x27;pre&#x27;</span>:<br>            xes = self.attention(xes, hidden, encoder_output, attn_mask)<br>        <span class="hljs-keyword">if</span> xes.dim() == <span class="hljs-number">2</span>:<br>            <span class="hljs-comment"># if only one token inputted, sometimes needs unsquezing</span><br>            xes.unsqueeze_(<span class="hljs-number">1</span>)<br>        output, new_hidden = self.rnn(xes, hidden)<br>        <span class="hljs-keyword">if</span> self.attn_time == <span class="hljs-string">&#x27;post&#x27;</span>:<br>            output = self.attention(output, new_hidden, encoder_output, attn_mask)<br><br>        <span class="hljs-keyword">if</span> self.numsoftmax &gt; <span class="hljs-number">1</span>:<br>            bsz = xs.size(<span class="hljs-number">0</span>)<br>            seqlen = xs.size(<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> xs.dim() &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span><br>            latent = self.latent(output)<br>            active = self.dropout(self.activation(latent))<br>            logit = self.e2s(active.view(-<span class="hljs-number">1</span>, self.esz))<br><br>            prior_logit = self.prior(output).view(-<span class="hljs-number">1</span>, self.numsoftmax)<br>            prior = self.softmax(prior_logit)  <span class="hljs-comment"># softmax over numsoftmax&#x27;s</span><br><br>            prob = self.softmax(logit).view(bsz * seqlen, self.numsoftmax, -<span class="hljs-number">1</span>)<br>            probs = (prob * prior.unsqueeze(<span class="hljs-number">2</span>)).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>).view(bsz, seqlen, -<span class="hljs-number">1</span>)<br>            scores = probs.log()<br><br>        <span class="hljs-keyword">else</span>:<br>            e = self.dropout(self.o2e(output))<br>            scores = self.e2s(e)<br><br>            <span class="hljs-comment"># select top scoring index, excluding the padding symbol (at idx zero)</span><br>            <span class="hljs-comment"># we can do topk sampling from renoramlized softmax here, default topk=1 is greedy</span><br>            <span class="hljs-keyword">if</span> topk == <span class="hljs-number">1</span>:<br>                _max_score, idx = scores.narrow(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, scores.size(<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>).<span class="hljs-built_in">max</span>(<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">elif</span> topk &gt; <span class="hljs-number">1</span>:<br>                max_score, idx = torch.topk(<br>                    F.softmax(scores.narrow(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, scores.size(<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>), <span class="hljs-number">2</span>),<br>                    topk,<br>                    dim=<span class="hljs-number">2</span>,<br>                    <span class="hljs-built_in">sorted</span>=<span class="hljs-literal">False</span>,<br>                )<br>                probs = F.softmax(<br>                    scores.narrow(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, scores.size(<span class="hljs-number">2</span>) - <span class="hljs-number">1</span>).gather(<span class="hljs-number">2</span>, idx), <span class="hljs-number">2</span><br>                ).squeeze(<span class="hljs-number">1</span>)<br>                dist = torch.distributions.categorical.Categorical(probs)<br>                samples = dist.sample()<br>                idx = idx.gather(-<span class="hljs-number">1</span>, samples.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)).squeeze(-<span class="hljs-number">1</span>)<br>            preds = idx.add_(<span class="hljs-number">1</span>)<br><br>            <span class="hljs-keyword">return</span> preds, scores, new_hidden<br></code></pre></td></tr></table></figure><h3 id="attention-module"><a class="markdownIt-Anchor" href="#attention-module"></a> Attention Module</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AttentionLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            attn_type,</span><br><span class="hljs-params">            hidden_size,</span><br><span class="hljs-params">            emb_size,</span><br><span class="hljs-params">            bidirectional=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">            attn_length=-<span class="hljs-number">1</span>,</span><br><span class="hljs-params">            attn_time=<span class="hljs-string">&#x27;pre&#x27;</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.attention = attn_type<br><br>        <span class="hljs-keyword">if</span> self.attention != <span class="hljs-string">&#x27;none&#x27;</span>:<br>            hsz = hidden_size<br>            hszXdirs = hsz * (<span class="hljs-number">2</span> <span class="hljs-keyword">if</span> bidirectional <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> attn_time == <span class="hljs-string">&#x27;pre&#x27;</span>:<br>                <span class="hljs-comment"># attention happens on the input embeddings</span><br>                input_dim = emb_size<br>            <span class="hljs-keyword">elif</span> attn_time == <span class="hljs-string">&#x27;post&#x27;</span>:<br>                <span class="hljs-comment"># attention happens on the output of the rnn</span><br>                input_dim = hsz<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;unsupported attention time&#x27;</span>)<br>            self.attn_combine = nn.Linear(hszXdirs + <span class="hljs-number">2</span> * input_dim, input_dim, bias=<span class="hljs-literal">False</span>)<br><br>            <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;local&#x27;</span>:<br>                <span class="hljs-comment"># local attention over fixed set of output states</span><br>                <span class="hljs-keyword">if</span> attn_length &lt; <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Set attention length to &gt; 0.&#x27;</span>)<br>                self.max_length = attn_length<br>                <span class="hljs-comment"># combines input and previous hidden output layer</span><br>                self.attn = nn.Linear(hsz + input_dim, attn_length, bias=<span class="hljs-literal">False</span>)<br>                <span class="hljs-comment"># combines attention weights with encoder outputs</span><br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;concat&#x27;</span>:<br>                self.attn = nn.Linear(hsz + hszXdirs, hsz, bias=<span class="hljs-literal">False</span>)<br>                self.attn_v = nn.Linear(hsz, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;general&#x27;</span>:<br>                <span class="hljs-comment"># equivalent to dot if attn is identity</span><br>                self.attn = nn.Linear(hsz, hszXdirs, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xes, hidden, enc_out, attn_mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;none&#x27;</span>:<br>            <span class="hljs-keyword">return</span> xes<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(hidden) == <span class="hljs-built_in">tuple</span>:<br>            <span class="hljs-comment"># for lstms use the &quot;hidden&quot; state not the cell state</span><br>            hidden = hidden[<span class="hljs-number">0</span>]<br>        last_hidden = hidden[-<span class="hljs-number">1</span>]  <span class="hljs-comment"># select hidden state from last RNN layer</span><br><br>        <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;local&#x27;</span>:<br>            <span class="hljs-keyword">if</span> enc_out.size(<span class="hljs-number">1</span>) &gt; self.max_length:<br>                offset = enc_out.size(<span class="hljs-number">1</span>) - self.max_length<br>                enc_out = enc_out.narrow(<span class="hljs-number">1</span>, offset, self.max_length)<br>            h_merged = torch.cat((xes.squeeze(<span class="hljs-number">1</span>), last_hidden), <span class="hljs-number">1</span>)<br>            attn_weights = F.softmax(self.attn(h_merged), dim=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> attn_weights.size(<span class="hljs-number">1</span>) &gt; enc_out.size(<span class="hljs-number">1</span>):<br>                attn_weights = attn_weights.narrow(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, enc_out.size(<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">else</span>:<br>            hid = last_hidden.unsqueeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> self.attention == <span class="hljs-string">&#x27;concat&#x27;</span>:<br>                hid = hid.expand(<br>                    last_hidden.size(<span class="hljs-number">0</span>), enc_out.size(<span class="hljs-number">1</span>), last_hidden.size(<span class="hljs-number">1</span>)<br>                )<br>                h_merged = torch.cat((enc_out, hid), <span class="hljs-number">2</span>)<br>                active = torch.tanh(self.attn(h_merged))<br>                attn_w_premask = self.attn_v(active).squeeze(<span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;dot&#x27;</span>:<br>                <span class="hljs-keyword">if</span> hid.size(<span class="hljs-number">2</span>) != enc_out.size(<span class="hljs-number">2</span>):<br>                    <span class="hljs-comment"># enc_out has two directions, so double hid</span><br>                    hid = torch.cat([hid, hid], <span class="hljs-number">2</span>)<br>                attn_w_premask = torch.bmm(hid, enc_out.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">elif</span> self.attention == <span class="hljs-string">&#x27;general&#x27;</span>:<br>                hid = self.attn(hid)<br>                attn_w_premask = torch.bmm(hid, enc_out.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># calculate activation scores</span><br>            <span class="hljs-keyword">if</span> attn_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># remove activation from NULL symbols</span><br>                attn_w_premask -= (<span class="hljs-number">1</span> - attn_mask) * <span class="hljs-number">1e20</span><br>            attn_weights = F.softmax(attn_w_premask, dim=<span class="hljs-number">1</span>)<br><br>        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="hljs-number">1</span>), enc_out)<br>        merged = torch.cat((xes.squeeze(<span class="hljs-number">1</span>), attn_applied.squeeze(<span class="hljs-number">1</span>)), <span class="hljs-number">1</span>)<br>        output = torch.tanh(self.attn_combine(merged).unsqueeze(<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> output<br><br></code></pre></td></tr></table></figure><h3 id="seq2seq-model-2"><a class="markdownIt-Anchor" href="#seq2seq-model-2"></a> Seq2Seq Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pad</span>(<span class="hljs-params">tensor, length, dim=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-keyword">if</span> tensor.size(dim) &lt; length:<br>        <span class="hljs-keyword">return</span> torch.cat(<br>            [<br>                tensor,<br>                tensor.new(<br>                    *tensor.size()[:dim],<br>                    length - tensor.size(dim),<br>                    *tensor.size()[dim + <span class="hljs-number">1</span> :],<br>                ).zero_(),<br>            ],<br>            dim=dim,<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> tensor<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2Seq</span>(nn.Module):<br>    RNN_OPTS = &#123;<span class="hljs-string">&#x27;rnn&#x27;</span>: nn.RNN, <span class="hljs-string">&#x27;gru&#x27;</span>: nn.GRU, <span class="hljs-string">&#x27;lstm&#x27;</span>: nn.LSTM&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, args, num_features, padding_idx=<span class="hljs-number">0</span>, start_idx=<span class="hljs-number">1</span>, end_idx=<span class="hljs-number">2</span>, longest_label=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.args = args<br><br>        self.attn_type = args.attn_type<br>        self.NULL_IDX = padding_idx<br>        self.END_IDX = end_idx<br>        self.register_buffer(<span class="hljs-string">&#x27;START&#x27;</span>, torch.LongTensor([start_idx]))<br>        self.longest_label = longest_label<br><br>        rnn_class = Seq2Seq.RNN_OPTS[args.rnn_class]<br>        self.decoder = Decoder(<br>            num_features,<br>            padding_idx=self.NULL_IDX,<br>            rnn_class=rnn_class,<br>            emb_size=args.embedding_size,<br>            hidden_size=args.hidden_size,<br>            num_layers=args.num_layers,<br>            dropout=args.dropout,<br>            attn_type=args.attn_type,<br>            attn_length=args.attn_length,<br>            attn_time=args.attn_time,<br>            bidir_input=args.bidirectional,<br>            numsoftmax=args.num_softmax,<br>            softmax_layer_bias=args.softmax_layer_bias<br>        )<br>        self.encoder = Encoder(<br>            num_features,<br>            padding_idx=self.NULL_IDX,<br>            rnn_class=rnn_class,<br>            emb_size=self.args.embedding_size,<br>            hidden_size=self.args.hidden_size,<br>            num_layers=self.args.num_layers,<br>            dropout=self.args.dropout,<br>            bidirectional=self.args.bidirectional,<br><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xs, ys=<span class="hljs-literal">None</span>, beam_size=<span class="hljs-number">1</span>, topk=<span class="hljs-number">1</span>, prev_enc=<span class="hljs-literal">None</span>, input_lens=<span class="hljs-literal">None</span>, res_lens=<span class="hljs-literal">None</span></span>):<br>        input_xs = xs<br>        nbest_beam_preds, nbest_beam_scores = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>        bsz = <span class="hljs-built_in">len</span>(xs)<br>        <span class="hljs-keyword">if</span> ys <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># keep track of longest label we&#x27;ve ever seen</span><br>            <span class="hljs-comment"># we&#x27;ll never produce longer ones than that during prediction</span><br>            self.longest_label = <span class="hljs-built_in">max</span>(self.longest_label, ys.size(<span class="hljs-number">1</span>))<br><br>        <span class="hljs-keyword">if</span> prev_enc <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            enc_out, hidden, attn_mask = prev_enc<br>        <span class="hljs-keyword">else</span>:<br>            enc_out, hidden = self.encoder(xs, input_lens)<br>            attn_mask = xs.ne(<span class="hljs-number">0</span>).<span class="hljs-built_in">float</span>() <span class="hljs-keyword">if</span> self.attn_type != <span class="hljs-string">&#x27;none&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        encoder_states = (enc_out, hidden, attn_mask)<br>        start = self.START.detach()<br>        starts = start.expand(bsz, <span class="hljs-number">1</span>)<br><br>        predictions = []<br>        scores = []<br>        cand_preds, cand_scores = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> ys <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            y_in = ys.narrow(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, ys.size(<span class="hljs-number">1</span>) - <span class="hljs-number">1</span>)<span class="hljs-comment"># y_in:(bs, tgt_len-1),ys:(bs, tgt_len) </span><br>            xs = torch.cat([starts, y_in], <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> self.attn_type == <span class="hljs-string">&#x27;none&#x27;</span>:<br>                preds, score, hidden = self.decoder(xs, hidden, enc_out, attn_mask)<br>                predictions.append(preds)<br>                scores.append(score)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(ys.size(<span class="hljs-number">1</span>)):<br>                    xi = xs.select(<span class="hljs-number">1</span>, i)<br>                    <span class="hljs-comment"># xi:(bs,), hidden:tuple(h_0:[2,bs,1024], c_0:[2,bs,1024])</span><br>                    <span class="hljs-comment"># 2=D*num_layers, if bi-RNN, D=2, else =1, 1024=H_out,H_cell</span><br>                    <span class="hljs-comment"># enc_out:(bs, src_len, 2048), attn_mask:(bs, src_len)</span><br>                    preds, score, hidden = self.decoder(xi, hidden, enc_out, attn_mask)<br>                    predictions.append(preds)<br>                    scores.append(score)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># here we do search: supported search types: greedy, beam search</span><br>            <span class="hljs-keyword">if</span> beam_size == <span class="hljs-number">1</span>:<br>                done = [<span class="hljs-literal">False</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(bsz)]<br>                total_done = <span class="hljs-number">0</span><br>                xs = starts<br><br>                <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.longest_label):<br>                    <span class="hljs-comment"># generate at most longest_label tokens</span><br>                    preds, score, hidden = self.decoder(<br>                        xs, hidden, enc_out, attn_mask, topk<br>                    )<br>                    scores.append(score)<br>                    xs = preds<br>                    predictions.append(preds)<br><br>                    <span class="hljs-comment"># check if we&#x27;ve produced the end token</span><br>                    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(bsz):<br>                        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> done[b]:<br>                            <span class="hljs-comment"># only add more tokens for examples that aren&#x27;t done</span><br>                            <span class="hljs-keyword">if</span> preds.data[b][<span class="hljs-number">0</span>] == self.END_IDX:<br>                                <span class="hljs-comment"># if we produced END, we&#x27;re done</span><br>                                done[b] = <span class="hljs-literal">True</span><br>                                total_done += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">if</span> total_done == bsz:<br>                        <span class="hljs-comment"># no need to generate any more</span><br>                        <span class="hljs-keyword">break</span><br><br>            <span class="hljs-keyword">elif</span> beam_size &gt; <span class="hljs-number">1</span>:<br>                enc_out, hidden = (<br>                    encoder_states[<span class="hljs-number">0</span>],<br>                    encoder_states[<span class="hljs-number">1</span>],<br>                )  <span class="hljs-comment"># take it from encoder</span><br>                enc_out = enc_out.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>                <span class="hljs-comment"># create batch size num of beams</span><br>                data_device = enc_out.device<br>                beams = [<br>                    Beam(<br>                        beam_size,<br>                        <span class="hljs-number">3</span>,<br>                        <span class="hljs-number">0</span>,<br>                        <span class="hljs-number">1</span>,<br>                        <span class="hljs-number">2</span>,<br>                        min_n_best=beam_size / <span class="hljs-number">2</span>,<br>                        cuda=data_device,<br>                    )<br>                    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(bsz)<br>                ]<br>                <span class="hljs-comment"># init the input with start token</span><br>                xs = starts<br>                <span class="hljs-comment"># repeat tensors to support batched beam</span><br>                xs = xs.repeat(<span class="hljs-number">1</span>, beam_size)<br>                attn_mask = input_xs.ne(<span class="hljs-number">0</span>).<span class="hljs-built_in">float</span>()<br>                attn_mask = attn_mask.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>)<br>                repeated_hidden = []<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(hidden, <span class="hljs-built_in">tuple</span>):<br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(hidden)):<br>                        repeated_hidden.append(<br>                            hidden[i].unsqueeze(<span class="hljs-number">2</span>).repeat(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>)<br>                        )<br>                    hidden = self.unbeamize_hidden(<br>                        <span class="hljs-built_in">tuple</span>(repeated_hidden), beam_size, bsz<br>                    )<br>                <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># GRU</span><br>                    repeated_hidden = hidden.unsqueeze(<span class="hljs-number">2</span>).repeat(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, beam_size, <span class="hljs-number">1</span>)<br>                    hidden = self.unbeamize_hidden(repeated_hidden, beam_size, bsz)<br>                enc_out = self.unbeamize_enc_out(enc_out, beam_size, bsz)<br>                xs = xs.view(bsz * beam_size, -<span class="hljs-number">1</span>)<br>                <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.longest_label):<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>((b.done() <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams)):<br>                        <span class="hljs-keyword">break</span><br>                    out = self.decoder(xs, hidden, enc_out)<br>                    scores = out[<span class="hljs-number">1</span>]<br>                    scores = scores.view(bsz, beam_size, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># -1 is a vocab size</span><br>                    <span class="hljs-keyword">for</span> i, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(beams):<br>                        b.advance(F.log_softmax(scores[i, :], dim=-<span class="hljs-number">1</span>))<br>                    xs = torch.cat(<br>                        [b.get_output_from_current_step() <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams]<br>                    ).unsqueeze(-<span class="hljs-number">1</span>)<br>                    permute_hidden_idx = torch.cat(<br>                        [<br>                            beam_size * i + b.get_backtrack_from_current_step()<br>                            <span class="hljs-keyword">for</span> i, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(beams)<br>                        ]<br>                    )<br>                    new_hidden = out[<span class="hljs-number">2</span>]<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(hidden, <span class="hljs-built_in">tuple</span>):<br>                        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(hidden)):<br>                            hidden[i].data.copy_(<br>                                new_hidden[i].data.index_select(<br>                                    dim=<span class="hljs-number">1</span>, index=permute_hidden_idx<br>                                )<br>                            )<br>                    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># GRU</span><br>                        hidden.data.copy_(<br>                            new_hidden.data.index_select(<br>                                dim=<span class="hljs-number">1</span>, index=permute_hidden_idx<br>                            )<br>                        )<br><br>                <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams:<br>                    b.check_finished()<br>                beam_pred = [<br>                    b.get_pretty_hypothesis(b.get_top_hyp()[<span class="hljs-number">0</span>])[<span class="hljs-number">1</span>:] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams<br>                ]<br>                <span class="hljs-comment"># these beam scores are rescored with length penalty!</span><br>                beam_scores = torch.stack([b.get_top_hyp()[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams])<br>                pad_length = <span class="hljs-built_in">max</span>([t.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> beam_pred])<br>                beam_pred = torch.stack(<br>                    [pad(t, length=pad_length, dim=<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> beam_pred], dim=<span class="hljs-number">0</span><br>                )<br><br>                <span class="hljs-comment">#  prepare n best list for each beam</span><br>                n_best_beam_tails = [<br>                    b.get_rescored_finished(n_best=<span class="hljs-built_in">len</span>(b.finished)) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> beams<br>                ]<br>                nbest_beam_scores = []<br>                nbest_beam_preds = []<br>                <span class="hljs-keyword">for</span> i, beamtails <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(n_best_beam_tails):<br>                    perbeam_preds = []<br>                    perbeam_scores = []<br>                    <span class="hljs-keyword">for</span> tail <span class="hljs-keyword">in</span> beamtails:<br>                        perbeam_preds.append(<br>                            beams[i].get_pretty_hypothesis(<br>                                beams[i].get_hyp_from_finished(tail)<br>                            )<br>                        )<br>                        perbeam_scores.append(tail.score)<br>                    nbest_beam_scores.append(perbeam_scores)<br>                    nbest_beam_preds.append(perbeam_preds)<br><br>                <span class="hljs-keyword">if</span> self.beam_log_freq &gt; <span class="hljs-number">0.0</span>:<br>                    num_dump = <span class="hljs-built_in">round</span>(bsz * self.beam_log_freq)<br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_dump):<br>                        dot_graph = beams[i].get_beam_dot(dictionary=self.<span class="hljs-built_in">dict</span>)<br>                        dot_graph.write_png(<br>                            os.path.join(<br>                                self.beam_dump_path,<br>                                <span class="hljs-string">&quot;&#123;&#125;.png&quot;</span>.<span class="hljs-built_in">format</span>(self.beam_dump_filecnt),<br>                            )<br>                        )<br>                        self.beam_dump_filecnt += <span class="hljs-number">1</span><br><br>                predictions = beam_pred<br>                scores = beam_scores<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(predictions, <span class="hljs-built_in">list</span>):<br>            predictions = torch.cat(predictions, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(scores, <span class="hljs-built_in">list</span>):<br>            scores = torch.cat(scores, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> (<br>            predictions,<br>            scores,<br>            cand_preds,<br>            cand_scores,<br>            encoder_states,<br>            nbest_beam_preds,<br>            nbest_beam_scores,<br>        )<br><br></code></pre></td></tr></table></figure><h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1><p>[1]Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems 27 (2014).<br>[2]Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
